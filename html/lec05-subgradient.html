<!doctype html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>/* copy from https://github.com/sindresorhus/github-markdown-css/ */

html,body{background-color: #342839;}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  line-height: 1.5;
  color: #fafedd;
  font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;
  font-size: 16px;
  line-height: 1.5;
  word-wrap: break-word;
}

.markdown-body .octicon {
  display: inline-block;
  fill: currentColor;
  vertical-align: text-bottom;
}

.markdown-body figure{margin:0;padding:0; display:table;}
.markdown-body figure figcaption{font-size:92%; text-align:center; color:#76dae8;}

.markdown-body .anchor {
  float: left;
  line-height: 1;
  margin-left: -20px;
  padding-right: 4px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #fed765;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1:hover .anchor .octicon-link:before,
.markdown-body h2:hover .anchor .octicon-link:before,
.markdown-body h3:hover .anchor .octicon-link:before,
.markdown-body h4:hover .anchor .octicon-link:before,
.markdown-body h5:hover .anchor .octicon-link:before,
.markdown-body h6:hover .anchor .octicon-link:before {
  width: 16px;
  height: 16px;
  content: ' ';
  display: inline-block;
  background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' width='16' height='16' aria-hidden='true'%3E%3Cpath fill-rule='evenodd' fill='%23fed765' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'%3E%3C/path%3E%3C/svg%3E");
}


.markdown-body details {
  display: block;
}

.markdown-body summary {
  display: list-item;
}

.markdown-body a {
  background-color: initial;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline-width: 0;
}

.markdown-body strong {
  font-weight: inherit;
  font-weight: bolder;
}
.markdown-body strong{
  color: #fe6188;
}
.markdown-body em{
  color: #77dbe8;
}

.markdown-body h1 {
  font-size: 2em;
  margin: .67em 0;
}

.markdown-body img {
  border-style: none;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace,monospace;
  font-size: 1em;
}

.markdown-body hr {
  box-sizing: initial;
  height: 0;
  overflow: visible;
}

.markdown-body input {
  font: inherit;
  margin: 0;
}

.markdown-body input {
  overflow: visible;
}

.markdown-body [type=checkbox] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body * {
  box-sizing: border-box;
}

.markdown-body input {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}

.markdown-body a {
  color: #aa9df2;
  text-decoration: none;
}
.markdown-body mjx-container[jax="SVG"] > svg a{fill:#aa9df2;stroke: #aa9df2;}

.markdown-body a:hover {
  text-decoration: underline;
}

.markdown-body strong {
  font-weight: 600;
}

.markdown-body hr:after,
.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body table {
  border-spacing: 0;
  border-collapse: collapse;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body details summary {
  cursor: pointer;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 12px SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  line-height: 12px;
  color: #76dae8;
  vertical-align: middle;
  background-color: #3a2e3f;
  border: 1px solid #504455;
  border-radius: 3px;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body h1 {
  font-size: 32px;
}

.markdown-body h1,
.markdown-body h2 {
  font-weight: 600;
}

.markdown-body h2 {
  font-size: 24px;
}

.markdown-body h3 {
  font-size: 20px;
}

.markdown-body h3,
.markdown-body h4 {
  font-weight: 600;
}

.markdown-body h4 {
  font-size: 16px;
}

.markdown-body h5 {
  font-size: 14px;
}

.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
}

.markdown-body h6 {
  font-size: 12px;
}

.markdown-body p {
  margin-top: 0;
  margin-bottom: 10px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ol ol ol,
.markdown-body ol ul ol,
.markdown-body ul ol ol,
.markdown-body ul ul ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre {
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body input::-webkit-inner-spin-button,
.markdown-body input::-webkit-outer-spin-button {
  margin: 0;
  -webkit-appearance: none;
  appearance: none;
}

.markdown-body:after,
.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  clear: both;
}

.markdown-body>:first-child {
  margin-top: 0!important;
}

.markdown-body>:last-child {
  margin-bottom: 0!important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body blockquote,
.markdown-body details,
.markdown-body dl,
.markdown-body ol,
.markdown-body p,
.markdown-body pre,
.markdown-body table,
.markdown-body ul {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: .25em;
  padding: 0;
  margin: 24px 0;
  background-color: #504455;
  border: 0;
}

.markdown-body blockquote {
  padding: 0 1em;
  color: #c9cdac;
  border-left: .25em solid #f5f5f5;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 24px;
  margin-bottom: 16px;
  font-weight: 600;
  line-height: 1.25;
}

.markdown-body h1 {
  font-size: 2em;
}

.markdown-body h1,
.markdown-body h2 {
  padding-bottom: .3em;
  border-bottom: 1px solid #4a3e4f;
  color: #fed765;
}

.markdown-body h2 {
  font-size: 1.5em;
  color: #fed765;
}

.markdown-body h3 {
  font-size: 1.25em;
  color: #fed765;
}

.markdown-body h4 {
  font-size: 1em;
  color: #fed765;
}

.markdown-body h5 {
  font-size: .875em;
  color: #fed765;
}

.markdown-body h6 {
  font-size: .85em;
  color: #fed765;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 2em;
}

.markdown-body ol ol,
.markdown-body ol ul,
.markdown-body ul ol,
.markdown-body ul ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li {
  word-wrap: break-all;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body li+li {
  margin-top: .25em;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: 600;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
}

.markdown-body table th {
  font-weight: 600;
}

.markdown-body table td,
.markdown-body table th {
  padding: 6px 13px;
  border: 1px solid #786c7d;
}

.markdown-body table tr {
  background-color: #342839;
  border-top: 1px solid #786c7d;
}

.markdown-body table th {
  background-color: #4a3e4f;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #392d3e;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: initial;
}

.markdown-body img[align=right] {
  padding-left: 20px;
}

.markdown-body img[align=left] {
  padding-right: 20px;
}

.markdown-body code {
  padding: .2em .4em;
  margin: 0;
  font-size: 85%;
  background-color: #3a2e3f;
  color: #76dae8;
  border-radius: 3px;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
   font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #3a2e3f;
  border-radius: 3px;
}

.markdown-body pre code {
  display: inline;
  max-width: auto;
  padding: 0;
  margin: 0;
  overflow: visible;
  line-height: inherit;
  word-wrap: normal;
  background-color: initial;
  border: 0;
  color: #f0f0f0;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 .2em .25em -1.6em;
  vertical-align: middle;
}
.markdown-body section.footnotes{
    margin-top:48px;
    border-top:solid 1px #504455;
    padding-top:0px;
}

@media (prefers-color-scheme: dark) {
  .markdown-body mark{color: #111;}
}

/* PrismJS 1.23.0
https://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */


code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background-color: #3a2e3f;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: #48be66;
}

.token.punctuation {
    color: #fdd664;
}

.token.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #9a95e3;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #fdd664;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #$$codeBlockColor$$;
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #ccddf6;
}

.token.function,
.token.class-name {
    color: #f28d55;
}

.token.regex,
.token.important,
.token.variable {
    color: #d38e63;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
  position: relative;
  padding-left: 3.8em;
  counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
  position: relative;
  white-space: inherit;
}

.line-numbers .line-numbers-rows {
  position: absolute;
  pointer-events: none;
  top: 0;
  font-size: 100%;
  left: -3.8em;
  width: 3em; /* works for line-numbers below 1000 lines */
  letter-spacing: -1px;
  border-right: 1px solid #726677;

  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;

}

  .line-numbers-rows > span {
    display: block;
    counter-increment: linenumber;
  }

    .line-numbers-rows > span:before {
      content: counter(linenumber);
      color: #726677;
      display: block;
      padding-right: 0.8em;
      text-align: right;
    }


</style><style>.mweb-charts{background:#fff;}
body{ box-sizing: border-box;
    margin: 0 auto;
    padding: 28px}
@media print{
    pre, code, pre code {
     overflow: visible !important;
     white-space: pre-wrap !important;       /* css-3 */
     white-space: -moz-pre-wrap !important;  /* Mozilla, since 1999 */
     white-space: -pre-wrap !important;      /* Opera 4-6 */
     white-space: -o-pre-wrap !important;    /* Opera 7 */
     word-wrap: break-word !important;       /* Internet Explorer 5.5+ */
    }
    html,body{margin:0;padding:4px;}
}

</style><script>window.MathJax = {     tex: { tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] },     startup: {     pageReady() {       return MathJax.startup.defaultPageReady().then(function () {          window.mweb_mathjax_ready_val = 'yes';          if(window.mweb_mathjax_ready !== undefined){ mweb_mathjax_ready(); }       });     }   }};document.addEventListener('DOMContentLoaded', function(event) {    if (typeof Prism != 'undefined') {         Prism.highlightAll();     }});window.mweb_mathjax_ready_val = '';function theMWebMathJaxRenderIsReady(key){ return window.mweb_mathjax_ready_val; }</script><script>window.MathJax = { tex: { tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] } }; </script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script></head><body><div id='markdown_content' class='markdown-body'><h1><a id="subgradient-methods" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subgradient Methods</h1>
<h2><a id="subgradient-and-subdifferential" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subgradient and Subdifferential</h2>
<ul>
<li><strong>Definition (Subgradient)</strong> Let \(f: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{+\infty\}\) be convex. A vector \(g \in \mathbb{R}^{n}\) is a <em>subgradient</em> of \(f\) at a point \(x \in \operatorname{dom}(f)\) if \(f(y) \geq f(x)+g^{\top}(y-x), \forall y \in \operatorname{dom}(f)\).
<ul>
<li>The set of all subgradient at \(x\) is called the <em>subdifferential</em> of \(f\) at \(x\) denoted as \(\partial f\).</li>
</ul>
</li>
<li><strong>Lemma 6.2 (uniqueness when differentiable)</strong> If \(f\) is convex and differentiable at \(x \in \operatorname{dom}(f)\) , then \(\partial f = \{\nabla f(x)\}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>Let \(y=x+\epsilon d\), if \(g\in\partial f\), then \(f(x+\epsilon d) \geq f(x)+\epsilon g^{\top} d\), then \(\frac{f(x+\epsilon d)-f(x)}{\epsilon} \geq g^{\top} d, \forall d, \forall \epsilon\).</li>
<li>Letting \(\epsilon \to 0\), then we have \(\nabla f(x)^{\top} d \geq g^{\top} d, \forall d\). This only holds when \(g = \nabla f(x)\).</li>
<li>If \(f\) convex, we can check the definition of subgradient holds.</li>
</ul>
</li>
<li><strong>Or</strong> can use Taylor expanssion and get \((\mathbf{g}-\nabla \mathbf{f}(\mathbf{x}))^{\top}(\mathbf{y}-\mathbf{x}) \leq \mathrm{r}(\mathbf{y}-\mathbf{x})\), then let \(\mathbf{y}=\mathbf{x}+\epsilon(\mathbf{g}-\nabla \mathbf{f}(\mathbf{x}))\).</li>
</ul>
</li>
<li><strong>Lemma 6.3</strong> If we don't assume convexity, we can only say \(\partial f(x) \subseteq\{\nabla f(x)\}\).</li>
<li><strong>Examples 6.4</strong>
<ol>
<li>\(f(x)=\frac{1}{2} x^{2}, \partial f(x)=x\)</li>
<li>\(f(x)=\vert x\vert, \partial f(x)=\left\{\begin{array}{l}\operatorname{sgn}(x), x \neq 0 \\{[-1,1], x=0}\end{array}\right.\)</li>
<li>\(f(x)=\left\{\begin{array}{l}-\sqrt{x}, x \geq 0 \\+\infty, o . w .\end{array} \quad, \partial f(0)=\emptyset ;\right.\)</li>
<li>\(f(x)=\left\{\begin{array}{l}1, x=0 \\0, x&gt;0 \\+\infty, o . w\end{array}, \partial f(0)=\emptyset\right.\)</li>
</ol>
</li>
<li><strong>Lemma 6.5</strong> Let \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) be convex, \(\operatorname{dom}(f)\) open, \(B \in \mathbb{R}_{+}\). Then \(\forall x\in\mathbf{dom}(d), \forall g\in\partial f(x), \Vert g\Vert_{a*}\leq B\) \(\Leftrightarrow\) \(\forall x,y\in\mathbf{dom}(d), \vert f(x)-f(y)\vert \leq B\Vert x-y\Vert_{a}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>(-&gt;)
<ul>
<li>Subgradient -&gt; \(f(y) - f(x) \leq g_y^{\top}(y-x)\) and \(f(x) - f(y) \leq g_x^{\top}(y-x)\)</li>
<li>This means \(\vert f(y) - f(x)\vert \leq \max\{ g_y^{\top}(y-x),  g_x^{\top}(y-x)\} \leq \max\{\Vert g_y\Vert_{a*},  \Vert g_x\Vert_{a*}\} \Vert y-x\Vert_{a}\)</li>
<li>If \(\Vert g\Vert_{a*} \leq B\) we arrive at Lipschitz</li>
</ul>
</li>
<li>(&lt;-)
<ul>
<li>\(f(y) \geq f(x) + g_x^{\top}(y-x)\), by Lipschitz, we have \(B\Vert x-y\Vert_{a} \geq g_x^{\top}(y-x), \forall y\). Choosing \(y-x = \arg\max_{\Vert y-x\Vert=1} g_x^{\top}(y-x)\), we get maximum of \(\Vert g_x\Vert_{a*}\leq B\). This is true forall \(x\).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="topological-properties-of-subgradients" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Topological Properties of Subgradients</h3>
<ul>
<li><strong>Lemma 6.6</strong> Let \(f(x)\) be a convex function and \(x \in \operatorname{dom}(f)\). Then \(\partial f(x)\) is convex and closed.
<ul>
<li><strong>Proof</strong> \(\partial f(x) = \cap_{y}\left\{g \in \mathbb{R}^{n}: f(y) \geq f(x)+g^{\top}(y-x)\right\}\), solution of \(g\) for each \(y\) is convex and closed linear separation.</li>
</ul>
</li>
<li><strong>Definition 6.7 (separation of convex sets)</strong> Let \(S\) and \(T\) be two nonempty convex sets in \(\mathbb{R}^n\). A hyperplane \(H=\left\{x \in \mathbb{R}^{n}: a^{\top} x=b, a\neq 0\right\}\) is said to <em>separate</em> \(S\) and \(T\) if \(S \cup T \not \subset H\) and \(S \subset H^{-}=\left\{x \in \mathbb{R}^{n}: a^{\top} x \leq b\right\}\) and \(T \subset H^{+}=\left\{x \in \mathbb{R}^{n}: a^{\top} x \geq b\right\}\).
<ul>
<li>PS: It's OK that one of \(S\) or \(T\) is a point, or part of the hyperplane.</li>
<li><em>Strictly separation</em> if \(S \subset H^{--}=\left\{x \in \mathbb{R}^{n}: a^{\top} x&lt;b\right\}\) and \(T \subset H^{++}=\left\{x \in \mathbb{R}^{n}: a^{\top} x&gt;b\right\}\).</li>
</ul>
</li>
<li><strong>Theorem 6.8 (Hyperplane separation theorem, [Roc97, Thm 11.3], no proof)</strong> Let \(S\) and \(T\) be two nonempty convex sets. Then S and T can be separated if and only if their (relative) interiors do not intersect, \(\operatorname{rint}(S) \cap \operatorname{rint}(T)=\emptyset\).
<ul>
<li><strong>PS</strong> The <em>Relative Interior</em> of a general non=convex set is defined to be \(\operatorname{relint}(C):=\{x \in C: \forall y \in C, \exists \lambda&gt;1, \text { s.t. } \lambda x+(1-\lambda) y \in C\}\)</li>
</ul>
</li>
<li><strong>Corollary 6.9</strong> Let \(S\) be a nonempty convex set and \(x_0\in\partial S\). \(\exists H=\left\{x: a^{\top} x=a^{\top} x_{0}\right\}, a\neq 0\) s.t. \(S \subset\left\{x: a^{\top} x \leq a^{\top} x_{0}\right\}, \) and \(x_{0} \in H\).
<ul>
<li>just let \(T=\{x_0\}\).</li>
</ul>
</li>
<li><strong>Theorem 6.10 (non emptiness of subgradient for interior)</strong> Let \(f(x)\) be a convex function and \(x \in \operatorname{rint}(\operatorname{dom}(f))\). Then \(\partial f(x)\) is nonempty and bounded if \(x \in \operatorname{rin} t(\operatorname{dom}(f))\).
<ul>
<li><strong>Proof</strong>
<ul>
<li><em>Non-emptiness</em>
<ul>
<li>W.l.o.g., assume \(\operatorname{dom}(f)\) is full-dimensional and \(x \in \operatorname{int}(\operatorname{dom}(f))\) (<em>interior</em>).</li>
<li>\(\mathrm{epi}(f)\) is convex and \((x, f(x))\) belongs to its boundary, by Thm 6.8 \(\exists \alpha=(s, \beta) \neq 0\) s.t. \(s^{\top} y+\beta t \geq s^{\mathcal{T}} x+\beta f(x), \forall(y, t) \in \operatorname{epi}(f)\)</li>
<li>We claim \(\beta &gt; 0\).
<ul>
<li>If \(\beta &lt; 0\), we can let \(t\to\infty\) where \((y, t) \in \operatorname{epi}(f)\).</li>
<li>if \(\beta = 0\),  this contradict to \(s^{\top} y \geq s^{\mathcal{T}} x\), since \(x\) is the interior, we can always find opposite direction to reverse the inequality.</li>
</ul>
</li>
<li>Setting \(g=-\beta^{-1} s\), we have \(f(y) \geq f(x)+g^{\top}(y-x), \forall y\)</li>
</ul>
</li>
<li><em>Boundedness</em>
<ul>
<li>If \(\partial f(x) \nless \infty\), \(\exists \{g_k\}_{k=1}^{\infty} \in \partial f(x)\) s.t. \(\lim_{k\to\infty}\Vert g_k\Vert_2 = \infty\).</li>
<li>Since \(x\) interior, exists ball \(B(x, \delta) \subseteq \mathbf{dom}(f)\), then \(y_{k}=x+\delta \frac{g_{k}}{\Vert g_{k}\Vert_{2}} \in \operatorname{dom}(f)\), by convexity, \(f\left(y_{k}\right) \geq f(x)+g_{k}^{\top}\left(y_{k}-x\right)=f(x)+\delta\Vert g_{k}\Vert_{2} \rightarrow \infty\), contradiction.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Lemma A (non-emptyness -&gt; convexity)</strong> If \(\forall x \in \operatorname{dom}(f), \partial f(x)\neq \emptyset\) and \(\mathbf{dom}(f)\) convex, then \(f\) convex.
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(z=\lambda x+(1-\lambda) y \in \operatorname{dom}(f)\) for \(\lambda \in [0,1]\).</li>
<li>Let \(g \in \partial f(z)\), we have \(f(x) \geq f(z)+g^{\top}(x-z), f(y) \geq f(z)+g^{\top}(y-z)\), add them with weight \((\lambda, 1-\lambda)\) we get convexity.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Remark 6.11 (Exercise 42)</strong> The subdifferential of a convex function \(f(x)\) at \(x \in \operatorname{dom}(f)\) is a monotone operator, i.e., \((u-v)^{\top}(x-y) \geq 0, \forall x, y \in \operatorname{dom}(f), u \in \partial f(x), v \in \partial f(y)\). <em>Proof similar to gradient</em>.</li>
</ul>
<h3><a id="subdifferential-and-directional-derivative" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subdifferential and directional derivative</h3>
<ul>
<li><strong>Def directional derivative</strong> \(f^{\prime}(x ; d)=\lim _{\delta \rightarrow 0^{+}} \frac{f(x+\delta d)-f(x)}{\delta}\).</li>
<li><strong>Def differentiable</strong> \(f\) <em>differentiable</em> if \(\Delta f = A \Delta x + o(\Delta x)\).
<ul>
<li>\(\partial f\) can exists when \(\mathrm{d} f\) not.</li>
</ul>
</li>
<li><strong>Lemma 6.12 (Exercise 43)</strong> Let \(f\) convex, the ratio \(\phi(\delta):=\frac{f(x+\delta d)-f(x)}{\delta}\) is non-decreasing of \(\delta&gt;0\).
<ul>
<li><strong>Proof</strong> Let \(\delta_1 \geq \delta_2\), then let \(\lambda := \delta_2/ \delta_1\), by convexity \(\phi(\delta_2) = \frac{f(x+\lambda \delta_1 d)-f(x)}{\lambda \delta_1} \leq \frac{\lambda f(x+ \delta_1 d) + (1-\lambda) f(x) -f(x)}{\lambda \delta_1} = \phi(\delta_1)\).</li>
</ul>
</li>
<li><strong>Theorem 6.13</strong>  Let \(f\) be convex and \(x \in \operatorname{int}(\operatorname{dom}(f))\), then \(f^{\prime}(x ; d)=\max _{g \in \partial f(x)} g^{\top} d\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>By def of subgrad, we have \(f^{\prime}(x ; d) \geq \max _{g \in \partial f(x)} g^{\top} d\).</li>
<li>Now we want to also prove the opposite.
<ul>
<li>Define \(C_{1}=\{(y, t): f(y)&lt;t\}\), (not \(\mathbf{epi}(f)\)), and \(C_{2}(d)=\left\{(y, t): y=x+\alpha d, t=f(x)+\alpha f^{\prime}(x ; d), \alpha \geq 0\right\}\).
<ul>
<li>so both \(C_1, C_2\) convex, non-empty.</li>
</ul>
</li>
<li>Since \(\phi(\delta)\) non-decreasing, \((t&gt;)\) \(f(x+\alpha d) \geq f(x)+\alpha f^{\prime}(x ; d), \forall \alpha \geq 0\), so \(C_{1} \cap C_{2}=\emptyset\)</li>
<li>By hyperplane separation theorem \(\exists\left(g_{0}, \beta\right) \neq 0\) s.t. \(g_{0}^{\top}(x+\alpha d)+\beta\left(f(x)+\alpha f^{\prime}(x ; d)\right) \leq g_{0}^{\top} y+\beta t, \forall \alpha \geq 0, \forall t&gt;f(y)\)
<ul>
<li>Similar to proof of Thm 6.10, we claim \(\beta &gt; 0\). Let \(\tilde{g}=\beta^{-1} g_{0}\)</li>
<li>we get \(\tilde{g}^{\top}(x+\alpha d)+f(x)+\alpha f^{\prime}(x ; d) \leq \tilde{g}^{\top} y+f(y), \forall \alpha \geq 0\).</li>
<li>Setting \(\alpha = 0\), we get \(\tilde{g}^{\top} x+f(x) \leq \tilde{g}^{\top} y+f(y) \Leftrightarrow-\tilde{g} \in \partial f(x)\)</li>
<li>setting \(\alpha = 1, y=x\), we get \(\tilde{g}^{\top} d+f^{\prime}(x ; d) \leq 0 \Leftrightarrow f^{\prime}(x ; d) \leq-\tilde{g}^{\top} d \leq \max _{g \in \partial f(x)} g^{\top} d\)</li>
</ul>
</li>
</ul>
</li>
<li>This mean \(f^{\prime}(x ; d) = \max _{g \in \partial f(x)} g^{\top} d\).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Theorem B [Roc97, Theorem 25.5] (differentiable almost everywhere)</strong> A (proper) convex function \(f\) is diﬀerentiable almost everywhere on (the interior of) \(\mathbf{dom}(f)\).</li>
<li><strong>Lemma C (optimal condition)</strong> If \(0 \in \partial f(x)\), then \(x\) is a global minimum. <strong>Proof</strong> \(f(\mathbf{y}) \geq f(\mathbf{x})+\mathbf{g}^{\top}(\mathbf{y}-\mathbf{x})=f(\mathbf{x})\).</li>
</ul>
<h3><a id="calculus-of-subgradient" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Calculus of Subgradient</h3>
<ul>
<li><strong>Conic combination</strong> \(h(x)=\lambda f(x)+\mu g(x)\), then \(\partial h(x)=\lambda \partial f(x)+\mu \partial g(x), \forall x \in \operatorname{int}(\operatorname{dom}(h))\).</li>
<li><strong>affine composition</strong> \(h(x)=f(A x+b)\), then \(\partial h(x)=A^{T} \partial f(A x+b)\).</li>
<li><strong>supremum</strong> \(h(x)=\sup _{\alpha \in \mathcal{A}} f_{\alpha}(x)\), then \(\partial h(x) \supseteq \operatorname{conv}\left\{\partial f_{\alpha}(x) \vert \alpha \in \alpha(x)\right\}\)</li>
<li><strong>superposition</strong> \(h(x)=F\left(f_{1}(x), \ldots, f_{m}(x)\right)\) where \(F\left(y_{1}, \ldots, y_{m}\right)\) non-decreasing and convex, then \(\partial h(x) \supseteq\left\{\sum_{i=1}^{m} d_{i} \partial f_{i}(x):\left(d_{1}, \ldots, d_{m}\right) \in \partial F\left(y_{1}, \ldots, y_{m}\right)\right\}\).</li>
<li><strong>Example 6.14</strong> Let \(h(x)=\max _{y \in C} f(x, y)\) where \(f(x,y)\) is convex in \(x\) for any \(y\) and \(C\) is closed. Then \(\partial f\left(x, y_{*}(x)\right) \subset \partial h(x)\), where \(y_{*}(x)=\operatorname{argmax}_{y \in C} f(x, y)\)
<ul>
<li><strong>Proof</strong> Let \(g \in \partial f\left(x, y_{*}(x)\right)\), then \(h(z) \geq f\left(z, y_{*}(x)\right) \geq f\left(x, y_{*}(x)\right)+g^{T}(z-x)=h(z)+g^{T}(z-x)\)</li>
</ul>
</li>
<li><strong>Lemma 6.15 (Exercise 44)</strong> Consider the function \(f(x)=\Vert x\Vert _{a}\). Then \(\partial f(x)=\left\{g: g^{\mathcal{T}} x=\Vert x\Vert _{a} \;\wedge\;\Vert g\Vert _{a*} \leq 1\right\}\). (In particular \(\partial f(0) = \left\{g:\Vert g\Vert _{a*} \leq 1\right\}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(g \in \partial f(x) \Leftrightarrow \forall \delta, \Vert x+\delta\Vert _{a} \geq \Vert x\Vert _{a} + g^{\top} \delta\)
<ul>
<li>setting \(\delta = -x\), we get \(0 = \Vert 0\Vert _{a} = \Vert x\Vert _{a} - g^{\top} x\).</li>
<li>By triangular ineq, \(\Vert x\Vert _{a} + \Vert \delta\Vert _{a} \geq \Vert x+\delta\Vert _{a} \geq \Vert x\Vert _{a} + g^{\top} \delta \Rightarrow \Vert \delta\Vert _{a} \geq g^{\top} \delta, \forall \delta\).
<ul>
<li>subjecting to \(\Vert \delta\Vert _{a}=1\) we get \(\Vert g\Vert _{a*} \leq 1\).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a id="subgradient-methods" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subgradient Methods</h2>
<ul>
<li><strong>Remark 6.16</strong> Negative subgradient may not be a descending direction.
<ul>
<li><em>Example</em> \(f\left(x_{1}, x_{2}\right)=\vert x_{1}\vert +2\vert x_{2}\vert\) at \(\mathbf{x}=(1,0), \partial f(\mathbf{x})=\{(1, a): a \in[-2,2]\}\)
<ul>
<li>\(\mathbf{g}=(1,0), \mathbf{d}=-\mathbf{g}\) is descending direction, while \(\mathbf{g}=(1,2), \mathbf{d}=-\mathbf{g}\) is not.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Problem</strong> \(\min f(x)\) s.t. \(x\in X\). <em>Denote</em>:
<ul>
<li>\(R:=\sqrt{\max _{x, y \in X}\Vert x-y\Vert _{2}^{2}}\) the diameter of \(X\).</li>
<li>\(B:=\sup _{x, y \in X} \frac{\vert f(x)-f(y)\vert }{\Vert x-y\Vert _{2}}&lt;+\infty\) the Lipschitz constant under \(\ell_2\) norm.</li>
</ul>
</li>
</ul>
<h3><a id="subgradient-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subgradient Descent</h3>
<ul>
<li><strong>Algorithm</strong> \(x_{t+1}=\Pi_{X}\left(x_{t}-\gamma_{t} g\left(x_{t}\right)\right)\), where \(g\left(x_{t}\right) \in \partial f\left(x_{t}\right)\).</li>
<li><strong>Lemma D(Descent Lemma)</strong> \(f\) convex, for any optimal \(\mathbf{x}^{*} \in X^{*}\), \(\lVert \mathbf{x}_{t+1}-\mathbf{x}^{*}\rVert _{2}^{2} \leq\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}^{2}-2 \gamma_{t}\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right)+\gamma_{t}^{2}\lVert \mathbf{g}_{t}\rVert _{2}^{2}\)
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(\Vert \mathbf{x}_{t+1} - \mathbf{x}^{\star}\Vert ^2_2 = \Vert \Pi_{X}\left(\mathbf{x}_{t}-\gamma_{t} \mathbf{g}_{t}\right) - \mathbf{x}^{\star}\Vert ^2_2\),</li>
<li>by Fact 4.1 (ii) \(\mathsf{RHS} \leq \Vert \mathbf{x}_{t}-\gamma_{t} \mathbf{g}_{t} - \mathbf{x}^{\star}\Vert ^2_2 =  \Vert \mathbf{x}_{t} - \mathbf{x}^{\star}\Vert ^2_2 - 2\gamma_t\langle \mathbf{g}_{t}, \mathbf{x}_{t} - \mathbf{x}^{\star} \rangle + \gamma_{t} ^2\Vert \mathbf{g}_{t}\Vert ^2_2\)</li>
<li>By convexity \(\langle \mathbf{g}_{t}, \mathbf{x}_{t} - \mathbf{x}^{\star} \rangle \leq f(\mathbf{x}_{t}) - f^{\star}\), then we arrive at conclusion.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="mathcal-o-1-sqrt-t-convergence-for-b-lipschitz-convex-functions" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(\mathcal{O}(1/\sqrt{t})\) convergence for \(B\)-Lipschitz convex functions</h3>
<ul>
<li>
<p><strong>Theorem 6.17</strong> \(f\) convex, then subgradient descent gives \(\max\{\min _{1 \leq t \leq T} f\left(\mathbf{x}_{t}\right), f\left(\hat{x}_{T}\right) \} - f^{*}\leq \frac{\lVert \mathbf{x}_{1}-\mathbf{x}^{*}\rVert _{2}^{2}+\sum_{t=1}^{T} \gamma_{t}^{2}\lVert \mathbf{g}_{t}\rVert _{2}^{2}}{2 \sum_{t=1}^{T} \gamma_{t}} \leq \frac{R^{2}+\sum_{t=1}^{T} \gamma_{t}^{2}B^2}{2 \sum_{t=1}^{T} \gamma_{t}}\).</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>By descent lemma \(\gamma_{t}\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right) \leq \frac{1}{2}\left( \lVert x_{t}-x^{*}\rVert _{2}^{2}-\lVert x_{t+1}-x^{*}\rVert _{2}^{2}+\gamma_{t}^{2}\lVert g\left(x_{t}\right)\rVert _{2}^{2} \right)\)</li>
<li>sum over \(t=[1:T]\) and divided by \(\sum_t \gamma_t\) gives upper bound of the average\(\mathbb{E} f_{t}\), after that is straightforward.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Step size</strong> is crucial for convergence behavior, unlike GD</p>
<ul>
<li><em>Constant Stepsize</em> \(\gamma_t \equiv \gamma\), \(\epsilon_{T} \leq \frac{R^{2}}{2 T} \cdot \frac{1}{\gamma}+\frac{B^{2}}{2} \gamma \stackrel{T \rightarrow \infty}{\longrightarrow} \frac{B^{2}}{2} \gamma\),
<ul>
<li>choosing \(\gamma_{*}=\frac{R}{B \sqrt{T}}\) yields \(\epsilon_{T} \leq \frac{R B}{\sqrt{T}}\).</li>
</ul>
</li>
<li><em>Scaled stepsize</em> \(\gamma_{t}=\frac{\gamma}{\lVert \mathbf{g}_{t}\rVert _{2}}\), \(\epsilon_{T} \stackrel{T \rightarrow \infty}{\longrightarrow}B \gamma / 2\)</li>
<li><em>Non-summable but diminishing stepsize</em> \(\sum_{t=1}^{\infty} \gamma_{t}=\infty,  \lim _{t \rightarrow \infty} \gamma_{t}=0\)
<ul>
<li>Since \(\gamma_t^2\) approach zero faster than \(\gamma_t\), \(\epsilon_{T} \stackrel{T \rightarrow \infty}{\longrightarrow} 0\).</li>
<li><em>Example</em> \(\gamma_{t}=O\left(t^{-q}\right)\) with \(q \in(0,1]\), e.g. \(q=1/2\), then \(\epsilon_{T} \leq O\left(\frac{B R \ln (T)}{\sqrt{T}}\right)\)
<ul>
<li>If we average among \([T/2:T]\), then \(\min _{\left\lfloor\frac{T}{2}\right\rfloor \leq t \leq T} f\left(x_{t}\right)-f^{*} \leq O\left(\frac{B R}{\sqrt{T}}\right)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><em>Non-summable but square-summable (Robbins-Monro) stepsize</em> \(\sum_{t=1}^{\infty} \gamma_{t}=\infty,  \sum_{t=1}^{\infty} \gamma_{t}^{2}&lt;\infty\), e.g. \(\gamma_t = t^{-1}\), \(\epsilon_{T} \stackrel{T \rightarrow \infty}{\longrightarrow} 0\).</p>
</li>
<li>
<p><em>Polyak stepsize</em> Assume \(f^{*}=f\left(x^{*}\right)\) is known and \(\gamma_{t}=\frac{f\left(x_{t}\right)-f^{*}}{\lVert g\left(x_{t}\right)\rVert _{2}^{2}}\)</p>
<ul>
<li>Using descent lemma, we get \(\lVert x_{t+1}-x^{*}\rVert _{2}^{2} \leq\lVert x_{t}-x^{*}\rVert _{2}^{2}-\frac{\left(f\left(x_{t}\right)-f^{*}\right)^{2}}{\lVert g\left(x_{t}\right)\rVert _{2}^{2}}\)</li>
<li>then \((f(x_t) - f(x^{\star}))^2 \leq \Vert \mathbf{g}_t\Vert ^2  ( \lVert x_{t}-x^{*}\rVert _{2}^2 - \lVert x_{t+1}-x^{*}\rVert _{2}^2 ) \leq B^2 (\lVert x_{t}-x^{*}\rVert _{2}^2 - \lVert x_{t+1}-x^{*}\rVert _{2}^2)\)</li>
<li>then \(\sum_{t=1}^{T}\left(f\left(x_{t}\right)-f^{*}\right)^{2} \leq R^{2} \cdot M&lt;\infty\), this means \(f\left(\mathbf{x}_{t}\right) \rightarrow f^{*}\).</li>
<li><strong>Note</strong> this is <em>last iterate</em> convergence, not <em>average</em> convergence!</li>
<li>can also show that \(\exists \mathbf{x}^{\star}\in X^{\star}\) s.t. \(\lim \sup _{t \rightarrow \infty}\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2} \rightarrow 0\), converge to one of the optimal point.
<ul>
<li>Since series \(\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}\) is bounded and non-increasing, there exists a subsequence \(\left\{\mathbf{x}_{t_{k}}\right\}\) with accumulation point \(\hat{\mathbf{x}}\),</li>
<li>Since we already have \(f\left(\mathbf{x}_{t}\right) \rightarrow f^{*}\), then \(\hat{\mathbf{x}} \in X^{*}\).</li>
<li>So \(\lVert \mathbf{x}_{t+1}-\hat{\mathbf{x}}\rVert _{2}^{2} \leq\lVert \mathbf{x}_{t_{k}}-\hat{\mathbf{x}}\rVert _{2}^{2}-\sum_{j=t_{k}}^{t} \frac{\left(f\left(\mathbf{x}_{j}\right)-f^{*}\right)^{2}}{\lVert g\left(\mathbf{x}_{j}\right)\rVert _{2}^{2}}\),</li>
<li>taking \(\lim\sup\) we have \(\lim\sup_{t\to\infty} \lVert \mathbf{x}_{t}-\hat{\mathbf{x}}\rVert _{2}^{2} \leq\lVert \mathbf{x}_{t_{k}}-\hat{\mathbf{x}}\rVert _{2}^{2}-\sum_{j=t_{k}}^{\infty} \frac{\left(f\left(\mathbf{x}_{j}\right)-f^{*}\right)^{2}}{\lVert g\left(\mathbf{x}_{j}\right)\rVert _{2}^{2}}\)</li>
<li>then take \(k\to\infty\), we get \(\limsup _{t \rightarrow \infty}\lVert \mathbf{x}_{t}-\hat{\mathbf{x}}\rVert _{2}^{2} \leq \lim _{k \rightarrow \infty}\lVert \mathbf{x}_{t_{k}}-\hat{\mathbf{x}}\rVert _{2}^{2}=0\).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="similar-result-under-polyak-s-step-size-exercise-prob-5" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Similar result under Polyak's step size (Exercise Prob 5)</h3>
<ul>
<li>For non-differentiable convex \(f\), \(\mu\)-strongly convex w.r.t. 2-norm is defined to be \(f_{\mu}(x):=f(x)-\frac{\mu}{2}\Vert \mathbf{x}\Vert ^{2}\) is convex. It can also be shown that this means \(f(y) \geq f(x)+g^{\top}(y-x)+\frac{\mu}{2}\Vert x-y\Vert ^{2}\).</li>
<li><strong>Lemma I</strong> If \(f\) convex and \(B\)-Lipschitz, optimization in whole space \(x \in \mathbb{R}^{d}\) with Polyak step size gives \(\min _{1 \leq t \leq T} f\left(x_{t}\right)-f\left(x^{*}\right) \leq \frac{B\lVert x_{1}-x^{*}\rVert _{2}}{\sqrt{T}}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>Denote \(h_t:=f\left(x_{t}\right)-f\left(x^{*}\right)\) and \(d_t:=\lVert x_{t}-x^{*}\rVert _{2}\), update gives \(d_{t+1}^{2} \leq d_{t}^{2}-\frac{h_{t}^{2}}{\lVert \mathbf{g}_{t}\rVert _{2}^{2}}\)</li>
<li>equivalently \(h_{t}^{2} \leq\lVert \mathbf{g}_{t}\rVert _{2}^{2}\left(d_{t}^{2}-d_{t+1}^{2}\right) \leq B^{2}\left(d_{t}^{2}-d_{t+1}^{2}\right)\)</li>
<li>sum over \(t=[1:T]\) gives the result.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Lemma J</strong> If \(f\) \(\mu\)-strongly convex and \(B\)-Lipschitz, optimization in whole space \(x \in \mathbb{R}^{d}\) with  Polyak step size gives \(\min _{1 \leq t \leq T} f\left(x_{t}\right)-f\left(x^{*}\right) \leq \frac{4 B^{2}}{\mu T}\)
<ul>
<li><strong>Proof</strong>
<ul>
<li>Similarly, \(d_{t+1}^{2} - d_{t}^{2} \leq -\frac{h_{t}^{2}}{\lVert \mathbf{g}_{t}\rVert _{2}^{2}} \leq  -\frac{h_{t}^{2}}{B^{2}} \)</li>
<li>By strong convexity w.r.t. optimial point (since optimization on whole space, \(0\in\partial f(\mathbf{x}^{\star})\), \(h_{t} = f(\mathbf{x}_t) - f(\mathbf{x}^{\star}) \leq \mu d_t^2/2\),
<ul>
<li>this means \(d_{t+1}^{2} - d_{t}^{2} \leq  -\frac{h_{t}^{2}}{B^{2}} \leq - \frac{\mu^2}{4B^2}d_t^4\). Denote \(a_{t}=\mu^{2}\lVert x_{t}-x^{*}\rVert _{2}^{2} /\left(4 B^{2}\right)\), we have \(a_{t+1} \leq a_t(1 - a_t)\)</li>
</ul>
</li>
<li>By strong convexity w.r.t. \(x_0\) and \(x^{\star}\), we have \(f^{\star} \geq f(x_0) + \mathbf{g}_0^{\top}(x^{\star} - x_0) + \mu \Vert \Delta x_0\Vert _2^2 / 2\)
<ul>
<li>by sufficient descent of Polyak step size, \(0 \geq - h_0 \geq \mu d_0^2/2 - \mathbf{g}_0^{\top}\Delta x_0\),</li>
<li>this means \(\mu d_0^2 /2 \geq \mathbf{g}_0^{\top}\Delta x_0 \leq B d_0\), take square we get \(a_0 \leq 1 \).</li>
</ul>
</li>
<li>We claim \(a_t \leq 1/(t+1)\), by induction, we have \(a_{t+1} \leq \frac{1}{t+2} \frac{t(t+2)}{(t+1)^2}\) (by monotonicity of \(x(1-x)\) in \([0, 1/2]\))
<ul>
<li>Since \(b_t := t/t+1\) is increasing, \(b_{t}/b_{t+1} = \frac{t(t+2)}{(t+1)^2} \leq 1\), this means \(a_{t+1} \leq \frac{1}{t+2}\), induction succeeds.</li>
<li>This means \(d_t^2 \leq \frac{4B^2}{\mu^2} \frac{1}{t+1} \leq \frac{4B^2}{\mu^2} \frac{1}{t} \)</li>
</ul>
</li>
<li>Again, by \(d_{t+1}^{2} - d_{t}^{2} \leq  -\frac{h_{t}^{2}}{B^{2}} \), sum over \(t\in[T/2, T]\), we get \(\sum_{t=T/2}^{T} h_t^2 \leq B^2 (d_{T/2}^2 - d^2_{T}) \leq  B^2 d_{T/2}^2 \leq \frac{8B^4}{\mu^2} \frac{1}{T}\)
<ul>
<li>this means \(\min_{t\in[T/2:T]} h_t^2 \leq \frac{16B^4}{\mu^2} \frac{1}{T^2}\), taking square root and we finish our proof.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="mathcal-o-1-t-convergence-for-strong-convex-functions" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(\mathcal{O}(1 / t)\) convergence for strong convex functions</h3>
<ul>
<li><strong>Lemma E(Descent Lemma under strong convexity)</strong> \(f\) is \(\mu\)-strongly convex, then \(\lVert \mathbf{x}_{t+1}-\mathbf{x}^{*}\rVert _{2}^{2} \leq\left(1-\mu \gamma_{t}\right)\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}^{2}-2 \gamma_{t}\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right)+\gamma_{t}^{2}\lVert \mathbf{g}_{t}\rVert _{2}^{2}\)
<ul>
<li><strong>Proof</strong> Replace \(\langle \mathbf{g}_{t}, \mathbf{x}_{t} - \mathbf{x}^{\star} \rangle \leq f(\mathbf{x}_{t}) - f^{\star}\) by \(\langle \mathbf{g}_{t}, \mathbf{x}_{t} - \mathbf{x}^{\star} \rangle \leq f(\mathbf{x}_{t}) - f^{\star} - (\mu/2)\Vert \mathbf{x}_{t} - \mathbf{x}^{\star}\Vert _2^2\).</li>
</ul>
</li>
<li>There are two choice of step size \(1/\mu t\) or \(2/\mu(t+1)\), which give different convergence rate.</li>
<li><strong>Theorem 6.18 (\(\mathcal{O}(\ln T/T)\))</strong> \(f\) is \(\mu\)-strongly convex and \(B\)-Lipschitz, then subgradient descent of step size \(\gamma_t = 1/\mu t\) gives \(\max\{\min _{1 \leq t \leq T} f\left(\mathbf{x}_{t}\right), f\left(\hat{x}_{T}\right) \} - f^{*} \leq \frac{B^{2}(\ln (T)+1)}{2 \mu T}\) where \(\hat{x}_{T}:=\frac{1}{T} \sum_{t=1}^{T} x_{t}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>Plug in \(\gamma = 1/\mu t\) we have \(f\left(x_{t}\right)-f^{*} \leq\left(\frac{\mu}{2}(t-1)\lVert x_{t}-x^{*}\rVert _{2}^{2}-\frac{\mu}{2} t\lVert x_{t+1}-x^{*}\rVert _{2}^{2}+\frac{1}{2 \mu t}\lVert g\left(x_{t}\right)\rVert _{2}^{2}\right)\),</li>
<li>sum over \(t=[1:T]\) we get \(\sum_{t=1}^{T}\left[f\left(x_{t}\right)-f^{*}\right] \leq \sum_{t=1}^{T} \frac{1}{2 \mu t}\lVert g\left(x_{t}\right)\rVert _{2}^{2} \leq \frac{B^{2}}{2 \mu} \sum_{t=1}^{T} \frac{1}{t} \leq \frac{B^{2}}{2 \mu}(\ln (T)+1)\)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Theorem F (\(\mathcal{O}(1/T)\))</strong> \(f\) is \(\mu\)-strongly convex and \(B\)-Lipschitz, then subgradient descent of step size \(\gamma_t = 2/\mu (t+1)\) gives \(\max\{\min _{1 \leq t \leq T} f\left(\mathbf{x}_{t}\right), f\left(\hat{x}_{T}\right) \} - f^{*} \leq \frac{2 B^{2}}{\mu \cdot(T+1)}\) where \(\hat{x}_{T}:=\sum_{t=1}^{T} t x_{t} / \sum_{t=1}^{T} t \).
<ul>
<li><strong>Proof</strong>
<ul>
<li>By descent lemma \(\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right) \leq \frac{1-\mu \gamma_{t}}{2 \gamma_{t}}\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}^{2}-\frac{1}{2 \gamma_{t}}\lVert \mathbf{x}_{t+1}-\mathbf{x}^{*}\rVert _{2}^{2}+\frac{\gamma_{t}}{2}\lVert \mathbf{g}_{t}\rVert _{2}^{2}\)</li>
<li>Plug in \(\gamma_t = 2/\mu (t+1)\) we get \(\mathsf{RHS} = \frac{\mu(t-1)}{4}\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}^{2}-\frac{\mu(t+1)}{4}\lVert \mathbf{x}_{t+1}-\mathbf{x}^{*}\rVert _{2}^{2}+\frac{1}{\mu(t+1)}\lVert \mathbf{g}_{t}\rVert _{2}^{2}\)</li>
<li>Times \(t\) on each side and we get \(t \left(f\left(\mathbf{x}_{t}\right)-f^{*}\right) \leq \frac{\mu t(t-1)}{4}\lVert \mathbf{x}_{t}-\mathbf{x}^{*}\rVert _{2}^{2}-\frac{\mu(t+1)t}{4}\lVert \mathbf{x}_{t+1}-\mathbf{x}^{*}\rVert _{2}^{2}+\frac{t}{\mu(t+1)}\lVert \mathbf{g}_{t}\rVert _{2}^{2}\)</li>
<li>Since \(\frac{t}{\mu(t+1)}\lVert \mathbf{g}_{t}\rVert _{2}^{2} \leq \frac{1}{\mu}B^{2}\), sum over \(t=[1:T]\) and get \(\sum_{t=1}^{T} t\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right) \leq-\frac{\mu T(T+1)}{4}\lVert \mathbf{x}_{T+1}-\mathbf{x}^{*}\rVert _{2}^{2}+\frac{T}{\mu} B^{2}\)</li>
<li>else is straightforward.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a id="lower-bound-complexity" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lower Bound Complexity</h2>
<p>We can show that, the above \(\mathcal{O}(1/\sqrt{t})\) and \(\mathcal{O}(1/t)\) can not be improved. The worst case function is given by the piecewise-linear function similar to \(f(x)=\max _{1 \leq i \leq n} x_{i}\).</p>
<ul>
<li><strong>Theorem 6.20 (Nemirovski &amp; Yudin 1979)</strong>. For any \(1 \leq t \leq n, x_{1} \in \mathbb{R}_{n}\),
<ul>
<li>(i) <em>\(B\)-Lipschit</em> there exists a \(B\)-Lipschitz continuous function \(f\) and a convex set \(X\) with diameter \(R\), such that for any first-order method that generates \(\mathbf{x}_{t} \in \mathbf{x}_{1}+\operatorname{span}\left(\mathbf{g}_{1}, \ldots, \mathbf{g}_{t-1}\right)\), where \(\mathbf{g}_{i} \in \partial f\left(\mathbf{x}_{i}\right)\), we have \(\min _{1 \leq s \leq t} f\left(\mathbf{x}_{s}\right)-f^{*} \geq \frac{B \cdot R}{4(1+\sqrt{t})}\).</li>
<li>(ii) <em>\(B\)-Lipschitz and \(\mu\)-convex</em> there exists a \(\mu\)-strongly convex, \(B\)-Lipschitz continuous function \(f\) and a convex set \(X\) with diameter \(R\), for any ﬁrst-order method as described above, we always have \(\min _{1 \leq s \leq t} f\left(x_{s}\right)-f^{*} \geq \frac{B^{2}}{8 \mu t}\).</li>
<li><strong>Proof</strong>
<ul>
<li>The function we construct is \(f(x)=C \cdot \max _{1 \leq i \leq t} x_{i}+\frac{\mu}{2}\Vert x\Vert _{2}^{2},\) and we restrict ourself within region \(X=\left\{x \in \mathbb{R}^{n},\Vert x\Vert _{2} \leq R\right\}\).
<ul>
<li>The subgradient is \(\partial f(x)=\mu x+C \cdot \operatorname{conv}\left\{e_{i}: i \text { such that } x_{i}=\max _{1 \leq j \leq t} x_{j}\right\}\).</li>
<li>The optimal solution is \(x_{i}^{*}=\begin{cases}-\frac{C}{\mu t} &amp; 1 \leq i \leq t \\0 &amp; t&lt;i \leq n\end{cases}\) and \(f^{*}=-\frac{C^{2}}{2 \mu t}\), can be verified by \(0 \in \partial f(x)\).</li>
</ul>
</li>
<li>Since all subgradient method controls \(\gamma_T\), but not how to choose subgradient, we can design a process that gives worst case. The update is design to have \(g(x)=C \cdot e_{i}+\mu x\), only one pure direction of \(x_{i}=\max _{1 \leq j \leq t} x_{j}\), but not its combination.
<ul>
<li>To simplify, we always choose the smallest one.</li>
</ul>
</li>
<li>We claim that if we start from \(x_{s=1} = 0\), then \(x_{s} \in \operatorname{span}\left(e_{1}, \ldots, e_{s-1}\right)\), always one dimension less than \(s\).
<ul>
<li>When \(s=2\), all coordinates are equal and we choose \(e_1\) to update, so \(x_{s=2} \in \mathrm{span}\{e_1\}\).</li>
<li>By our update rule \(g(x)=C \cdot e_{i}+\mu x\), the span can only increase at most one dimension at each update, so induction still holds.</li>
</ul>
</li>
<li>This means for \(x_s, 1\leq s \leq t\), at least one coordinate is unchanged as \(0\). So \(\max _{1 \leq i \leq t} x_{i}\) -&gt; \(f(x_s) \geq 0\).
<ul>
<li>This gives \(\min _{1 \leq s \leq t} f\left(x_{s}\right)-f^{*} \geq \frac{C^{2}}{2 \mu t}\)</li>
</ul>
</li>
<li>Choosing \(C=\frac{B \sqrt{t}}{1+\sqrt{t}}, \mu=\frac{2 B}{R(1+\sqrt{t})}\), we have \(\Vert \partial f(x)\Vert _{2} \leq C+\mu\Vert x\Vert _{2} \leq C+\mu R=: M\), this is the \(M\)-Lipschitz case, we have \(\min _{1 \leq s \leq t} f\left(x_{s}\right)-f^{*} \geq \frac{C^{2}}{2 \mu t}=\frac{B \cdot R}{2(1+\sqrt{t})}\).</li>
<li>Choosing \(C=\frac{B}{2}, \mu=\frac{B}{R}\), we have \(\Vert \partial f(x)\Vert _{2} \leq C+\mu R=:M\), this is \(M\)-Lispchitz and \(\mu\)-strongly convex case, we have \(\min _{1 \leq s \leq t} f\left(x_{s}\right)-f^{*} \geq \frac{C^{2}}{2 \mu t}=\frac{B^{2}}{8 \mu t}\)</li>
</ul>
</li>
<li><em>Comment</em> This theorem is so strange that number of updates is related to dimension.</li>
</ul>
</li>
</ul>
<h2><a id="mirror-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mirror Descent</h2>
<ul>
<li><strong>Key Idea</strong> Update can be viewed as \(x_{t+1}=\underset{x \in X}{\operatorname{argmin}}\left\{\frac{1}{2}\lVert x-x_{t}\rVert _{2}^{2}+\left\langle\gamma_{t} g\left(x_{t}\right), x\right\rangle\right\}\), how about we change \(\Vert \cdot\Vert _2^2\) to something else.</li>
</ul>
<h3><a id="bregman-divergence" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bregman Divergence</h3>
<ul>
<li><strong>Definition 6.22</strong> Let \(\omega(x): X \rightarrow \mathbb{R}\) be a function that is <em>strictly</em> convex, continuously differentiable on a closed convex set \(X\). The <em>Bregman divergence</em> is defined as \(V_{\omega}(x, y)=\omega(x)-\omega(y)-\nabla \omega(y)^{T}(x-y), \forall x, y \in X\).
<ul>
<li>Asymmetric \(V_{\omega}(x, y) \neq V_{\omega}(y, x)\), not a valid distance, triangle inequality may not hold.</li>
<li>\(\omega(\cdot)\) is called <em>distance-generating function</em>.</li>
<li>If \(\omega(\cdot)\) is also \(\sigma\)-strongly convex w.r.t. norm \(\Vert \cdot\Vert _{a}\), then \(V_{\omega}(x,y) \geq \sigma \Vert x-y\Vert _{a}^2 /2\).</li>
</ul>
</li>
<li><strong>Example</strong>
<ul>
<li><em>Euclidean Distance</em> \(\Omega=\mathbb{R}^{d}, \omega(\mathbf{x})=\frac{1}{2}\Vert \mathbf{x}\Vert _{2}^{2}\), \(\omega\) is \(1\) strongly convex w.r.t \(\ell_2\) norm,
<ul>
<li>\(V_{\omega}(\mathbf{x}, \mathbf{y})=\frac{1}{2}\Vert \mathbf{x}-\mathbf{y}\Vert _{2}^{2}\).</li>
</ul>
</li>
<li><em>Mahalanobis distance</em> \(\Omega=\mathbb{R}^{d}, \omega(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}\) where \(Q \succeq I\), \(\omega\) is \(1\) strongly convex w.r.t \(\ell_2\) norm,
<ul>
<li>\(V_{\omega}(\mathbf{x}, \mathbf{y})=\frac{1}{2}(\mathbf{x}-\mathbf{y})^{T} Q(\mathbf{x}-\mathbf{y})\).</li>
</ul>
</li>
<li><em>Kullback-Leibler divergence</em> \(\Omega=\Delta_{d}, \omega(\mathbf{x})=\sum_{i=1}^{d} x_{i} \ln x_{i}\), \(\nabla \omega (x) = \ln (x) - 1\),
<ul>
<li>\(V_{\omega}(x,y) = \sum_{i} x_i \ln x_i - y_i \ln y_i - (\ln(y_i) - 1)(x_i - y_i) = \sum_i x_i \ln \frac{x_i}{y_i} = \mathrm{KL}(x \Vert  y)\), since \(\sum_i x_i = \sum_i y_i =1\).</li>
<li>The proof for \(V_{\omega}(x,y) \geq \Vert x-y\Vert _1^2 /\ln 4\) is complicated, see <a href="http://www.stat.yale.edu/~yw562/teaching/598/lec04.pdf">this</a>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Lemma 6.23 (Generalized Pythagorean Theorem, Exercise 46)</strong> If \(x^{*}\) is the Bregman projection of \(x_0\) onto a convex set \(C \subset X: x^{*} \operatorname{argmin}_{x \in C} V_{\omega}\left(x, x_{0}\right)\). Then \(\forall y \in C\), \(V_{\omega}\left(y, x_{0}\right) \geq V_{\omega}\left(y, x^{*}\right)+V_{\omega}\left(x^{*}, x_{0}\right)\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(\nabla_x V_\omega(x,y) = \nabla \omega(x) - \nabla \omega(y)\), optimal condition means \(\forall y \in C, (\nabla \omega(x^{*}) - \nabla \omega(x_0))^{\top}(y - x^{*}) \geq 0\).</li>
<li>equivalently, \(- \nabla \omega(x_0)^{\top}(y - x_0 + x_0 -x^{*}) \geq -\nabla \omega(x^{*})^{\top}(y - x^{*})\)</li>
<li>or \(- \nabla \omega(x_0)^{\top}(y - x_0) \geq -\nabla \omega(x^{*})^{\top}(y - x^{*}) - \nabla \omega(x_0)^{\top}(x_0 -x^{*})\)</li>
<li>then \(\omega(x_0)  - \omega_(y)- \nabla \omega(x_0)^{\top}(y - x_0) \geq \omega(x_0)  - \omega_(x^{*}) +\omega(x^{*})- \omega_(y) -  \nabla \omega(x^{*})^{\top}(y - x^{*}) +- \nabla \omega(x_0)^{\top}(x_0 -x^{*})\), QED.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="mirror-descent-algorithm" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mirror Descent Algorithm</h3>
<ul>
<li><em>Prox-mapping</em> \(\operatorname{Prox}_{x}(\xi)=\underset{u \in X}{\operatorname{argmin}}\left\{V_{\omega}(u, x)+\langle\xi, u\rangle\right\}\), and suppose \(\omega\) is \(1\)-strongly convex on norm \(\Vert \cdot\Vert _{a}\).</li>
<li><strong>Algorithm</strong> \(x_{t+1}=\operatorname{Prox}_{x_{t}}\left(\gamma_{t} g\left(x_{t}\right)\right)\), where \(g\left(x_{t}\right) \in \partial f\left(x_{t}\right)\).</li>
<li><strong>Example 6.25</strong> Under KL divergence, the prox-mapping becomes \(\operatorname{Prox}_{x}\xi)=\left(\sum_{i=1}^{n} x_{i} e^{-\xi_{i}}\right)^{-1}\left[\begin{array}{c}x_{1} e^{-\xi_{1}} \\\ldots \\x_{n} e^{-\xi_{n}}\end{array}\right]\)
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(V_{\omega}(u, x)+\langle\xi, u\rangle = \sum_{i} u_i \ln (u_i / x_i) + u_i \xi_i\), optimal condition under constraint is \(0 = \ln(u_i/x_i) + 1 + \lambda + \xi_i\)</li>
<li>So solution is \(u_i = x_i e^{-\xi_i + \alpha}\) where \(\sum_i u_i =1 \Rightarrow e^{-\alpha} = \sum_i x_i e^{-\xi_i}\) QED.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="mathcal-o-1-sqrt-t-convergence-for-convex-functions" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(\mathcal{O}(1 / \sqrt{t})\) convergence for convex functions</h3>
<ul>
<li>
<p><strong>Lemma 6.26 (Three point identity)</strong> \(\forall x,y,z \in \mathbf{dom}(\omega)\), \(V_{\omega}(x, z)=V_{\omega}(x, y)+V_{\omega}(y, z)-\langle\nabla \omega(z)-\nabla \omega(y), x-y\rangle\)</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(V_{\omega}(x, y)+V_{\omega}(y, z)=\omega(x)-\omega(y)+\omega(y)-\omega(z)-\langle\nabla \omega(y), x-y\rangle-\langle\nabla \omega(z), y-z\rangle =V_{\omega}(x, z)+\langle\nabla \omega(z), x-z\rangle-\langle\nabla \omega(y), x-y\rangle-\langle\nabla \omega(z), y-z\rangle =V_{\omega}(x, z)+\langle\nabla \omega(z)-\nabla \omega(y), x-y\rangle\)</li>
</ul>
</li>
<li>When \(\omega = \Vert \cdot\Vert _2^2\), this becomes law of cosines.</li>
</ul>
</li>
<li>
<p><strong>Lemma G (Descent Lemma)</strong> \(\gamma_{t}\left(f\left(\mathbf{x}_{t}\right)-f^{*}\right) \leq V_{\omega}\left(\mathbf{x}^{*}, \mathbf{x}_{t}\right)-V_{\omega}\left(\mathbf{x}^{*}, \mathbf{x}_{t+1}\right)+\frac{\gamma_{t}^{2}}{2}\lVert \mathbf{g}_{t}\rVert _{a*}^{2}\)</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>By 3-point identiy, \(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t})=V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1})+V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t})-\langle\nabla \omega(\mathbf{x}_{t})-\nabla \omega(\mathbf{x}_{t+1}), \mathbf{x}^{\star}-\mathbf{x}_{t+1}\rangle\), equivalently
<ul>
<li>\(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1}) + \langle\nabla \omega(\mathbf{x}_{t})-\nabla \omega(\mathbf{x}_{t+1}), \mathbf{x}^{\star}-\mathbf{x}_{t+1}\rangle = V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t})\)</li>
</ul>
</li>
<li>by def of algo, the optimial condition is \(\forall x\in X\), espectially \(\mathbf{x}^{\star}\) \(\langle \nabla_{\mathbf{x}_{t+1}}V_{\omega}(\mathbf{x}_{t+1},\mathbf{x}_{t}) + \gamma_t \mathbf{g}_{t},\mathbf{x}^{\star} - \mathbf{x}_{t+1}\rangle=  \langle \nabla \omega (\mathbf{x}_{t+1}) - \nabla \omega (\mathbf{x}_{t}) + \gamma_t \mathbf{g}_{t},\mathbf{x}^{\star} - \mathbf{x}_{t+1}\rangle \geq 0\), this means
<ul>
<li>\(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1}) + \langle\gamma_t \mathbf{g}_{t}, \mathbf{x}^{\star}-\mathbf{x}_{t+1}\rangle \geq V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t})\)</li>
</ul>
</li>
<li>By convexity, \(\langle\mathbf{g}_{t}, \mathbf{x}^{\star}-\mathbf{x}_{t}\rangle \leq f(\mathbf{x}^{\star}) - f(\mathbf{x}_{t})\), so \(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1}) +\gamma_t ( f(\mathbf{x}^{\star}) - f(\mathbf{x}_{t}))  \geq V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) + \langle\gamma_t \mathbf{g}_{t}, \mathbf{x}_{t+1} - \mathbf{x}_{t}\rangle\)</li>
<li>By <em>Young’s inequality</em>, \(\langle a, b\rangle  \leq \Vert a\Vert _{a}\cdot \Vert b\Vert _{a*} \leq \Vert a\Vert _{a}^2/2 + \Vert b\Vert _{a*}^2/2\), therefore
<ul>
<li>\(\langle\gamma_t \mathbf{g}_{t}, \mathbf{x}_{t} - \mathbf{x}_{t+1}\rangle \leq \gamma_t^2 \Vert \mathbf{g}_t\Vert _{a*}^2/2 + \Vert \mathbf{x}_{t} - \mathbf{x}_{t+1}\Vert ^2_{a}/2\)</li>
</ul>
</li>
<li>By the assumption of \(\omega\) is \(1\)-strongly convex, \(V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) \geq  \Vert \mathbf{x}_{t} - \mathbf{x}_{t+1}\Vert ^2_{a}/2\)</li>
<li>Combine the above two, \(V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) + \langle\gamma_t \mathbf{g}_{t}, \mathbf{x}_{t+1} - \mathbf{x}_{t}\rangle \geq \Vert \mathbf{x}_{t} - \mathbf{x}_{t+1}\Vert ^2_{a}/2 - \gamma_t^2 \Vert \mathbf{g}_t\Vert _{a*}^2/2 - \Vert \mathbf{x}_{t} - \mathbf{x}_{t+1}\Vert ^2_{a}/2 = - \gamma_t^2 \Vert \mathbf{g}_t\Vert _{a*}^2/2\)</li>
<li>Combine above, we arrive at the descent lemma.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Theorem 6.28</strong> \(f\) convex, then \(\displaystyle \max\left\{\min _{1 \leq t \leq T} f\left(x_{t}\right), f\left(\hat{x}_{T}\right)\right\}-f^{*} \leq \frac{V_{\omega}\left(x^{*}, x_{1}\right)+\frac{1}{2} \sum_{t=1}^{T} \gamma_{t}^{2}\lVert g\left(x_{t}\right)\rVert _{*}^{2}}{\sum_{t=1}^{T} \gamma_{t}}\), where \(\displaystyle \hat{x}_T = \frac{\sum_{t=1}^{T} \gamma_{t} x_{t}}{\sum_{t=1}^{T} \gamma_{t}}\).</p>
<ul>
<li><strong>Proof</strong> Similar to Theorem 6.17, sum over all \(t\in[1:T]\) we get the result.</li>
<li>Convergence similar to subgradient case \(\min _{1 \leq t \leq T} f\left(\mathbf{x}_{t}\right)-f^{*}=O\left(\frac{B R}{\sqrt{T}}\right)\), where \(R=\sqrt{\max _{\mathbf{x} \in X} V_{\omega}\left(\mathbf{x}, \mathbf{x}_{1}\right)}\) and \(B:=\sup _{\mathbf{x} \in X} \frac{\vert f(\mathbf{x})-f(\mathbf{y})\vert }{\Vert \mathbf{x}-\mathbf{y}\Vert }\).</li>
</ul>
</li>
<li>
<p>The constant may be different. <strong>Example of Simplex</strong> of \(X=\left\{x \in \mathbb{R}^{d}: x_{i} \geq 0, \sum_{i=1}^{d} x_{i}=1\right\}\), assume \(\Vert \mathbf{g}\Vert _{\infty} \leq 1\),</p>
<ul>
<li>In normal case of \(\omega(x)=\frac{1}{2}\Vert x\Vert _{2}^{2}\), \(R^2 \leq 2= O(1)\), \(B \sim \Vert g\Vert _2 = O(\sqrt{d})\), overall it's \(O(\frac{\sqrt{d}}{\sqrt{T}})\).</li>
<li>If we choose \(w(x)=\sum_{i=1}^{d} x_{i} \ln x_{i}\) and starting point to be \(x_{1}=\operatorname{argmin}_{x \in X} \omega(x)\), then \(\Omega \leq \max _{x \in X} \omega(x)-\min _{x \in X} \omega(x)=0-(-\ln d)=\ln (d)\), overall it's \(O(\frac{\ln d}{\sqrt{T}})\)</li>
<li>The ratio of efficiency is then \(O\left(\frac{1}{\ln (d)} \cdot \frac{\max _{x \in X}\Vert g(x)\Vert _{2}}{\max _{x \in X}\Vert g(x)\Vert _{\infty}}\right)\), since \(\Vert g(x)\Vert _{\infty} \leq\Vert g(x)\Vert _{2} \leq \sqrt{d}\Vert g(x)\Vert _{\infty}\), in worst case Mirror Descent is \(O(\sqrt{d})\) faster than norm subgradient descent.</li>
</ul>
</li>
</ul>
<h3><a id="mirror-descent-under-smoothness-exercise-47" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mirror Descent under Smoothness (Exercise 47)</h3>
<ul>
<li><strong>Lemma H</strong> If \(f\) convex and gradient Lipschitz w.r.t. some norm, \(\Vert \nabla f(x)-\nabla f(y)\Vert _{a*} \leq L\Vert x-y\Vert _{a}\), then setting \(\gamma_{t}=1 / L\) will give \(\min _{1 \leq t \leq T} f\left(x_{t}\right)-f^{*} \leq {L \cdot V_{\omega}\left(x^{*}, x_{1}\right)}/{T}\)
<ul>
<li>
<p><strong>Proof</strong></p>
<ul>
<li>By equivalence of smoothness and Lipschitz, \(f(\mathbf{x}_{t+1}) \leq f(\mathbf{x}_{t})+\mathbf{g}_{t}^{\top}(\mathbf{x}_{t+1}-\mathbf{x}_{t})+\frac{L}{2}\Vert \mathbf{x}_{t}-\mathbf{x}_{t+1}\Vert _{a}^{2}\)
<ul>
<li>Using the fact of \(\omega\) is \(1\)-strongly convex, we have \(\Vert \mathbf{x}_{t}-\mathbf{x}_{t+1}\Vert _{a}^{2}/2 \leq V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t})\)</li>
<li>This makes \(\frac{1}{\gamma_t} V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) + \langle \mathbf{g}_{t}, \mathbf{x}_{t+1}-\mathbf{x}_{t} \rangle \geq f(\mathbf{x}_{t+1}) - f(\mathbf{x}_{t})\)</li>
<li>By the optimal condition of prox-mapping, we can show that \(V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) + \langle \gamma_{t} \mathbf{g}_{t}, \mathbf{x}_{t+1} \rangle \leq \underset{=0}{V_{\omega}(\mathbf{x}_{t}, \mathbf{x}_{t})} + \langle \gamma_{t} \mathbf{g}_{t}, \mathbf{x}_{t} \rangle\)</li>
<li>This means \(f(\mathbf{x}_{t+1}) - f(\mathbf{x}_{t}) \leq 0\), always non-increasing.</li>
</ul>
</li>
<li>In the third step of descent lemma, bu convexity we have \(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1}) +\gamma_t ( f(\mathbf{x}^{\star}) - f(\mathbf{x}_{t}))  \geq V_{\omega}(\mathbf{x}_{t+1}, \mathbf{x}_{t}) + \langle\gamma_t \mathbf{g}_{t}, \mathbf{x}_{t+1} - \mathbf{x}_{t}\rangle\)
<ul>
<li>take the above into it we get \(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{t+1}) + \gamma_t ( f(\mathbf{x}^{\star}) - f(\mathbf{x}_{t})) \geq \gamma_t(f(\mathbf{x}_{t+1}) - f(\mathbf{x}_{t}))\)</li>
</ul>
</li>
<li>take \(\gamma_t = L^{-1}\) and sum over \(t=[0:T-1]\), we have
<ul>
<li>\(\sum_{i=1}^{T} f(\mathbf{x}_{i}) - f(\mathbf{x}^{\star}) \leq L\left(V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{1}) - V_{\omega}(\mathbf{x}^{\star}, \mathbf{x}_{T})\right)\), QED</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></body></html>