<!doctype html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>/* copy from https://github.com/sindresorhus/github-markdown-css/ */

html,body{background-color: #342839;}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  line-height: 1.5;
  color: #fafedd;
  font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;
  font-size: 16px;
  line-height: 1.5;
  word-wrap: break-word;
}

.markdown-body .octicon {
  display: inline-block;
  fill: currentColor;
  vertical-align: text-bottom;
}

.markdown-body figure{margin:0;padding:0; display:table;}
.markdown-body figure figcaption{font-size:92%; text-align:center; color:#76dae8;}

.markdown-body .anchor {
  float: left;
  line-height: 1;
  margin-left: -20px;
  padding-right: 4px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #fed765;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1:hover .anchor .octicon-link:before,
.markdown-body h2:hover .anchor .octicon-link:before,
.markdown-body h3:hover .anchor .octicon-link:before,
.markdown-body h4:hover .anchor .octicon-link:before,
.markdown-body h5:hover .anchor .octicon-link:before,
.markdown-body h6:hover .anchor .octicon-link:before {
  width: 16px;
  height: 16px;
  content: ' ';
  display: inline-block;
  background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' width='16' height='16' aria-hidden='true'%3E%3Cpath fill-rule='evenodd' fill='%23fed765' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'%3E%3C/path%3E%3C/svg%3E");
}


.markdown-body details {
  display: block;
}

.markdown-body summary {
  display: list-item;
}

.markdown-body a {
  background-color: initial;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline-width: 0;
}

.markdown-body strong {
  font-weight: inherit;
  font-weight: bolder;
}
.markdown-body strong{
  color: #fe6188;
}
.markdown-body em{
  color: #77dbe8;
}

.markdown-body h1 {
  font-size: 2em;
  margin: .67em 0;
}

.markdown-body img {
  border-style: none;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace,monospace;
  font-size: 1em;
}

.markdown-body hr {
  box-sizing: initial;
  height: 0;
  overflow: visible;
}

.markdown-body input {
  font: inherit;
  margin: 0;
}

.markdown-body input {
  overflow: visible;
}

.markdown-body [type=checkbox] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body * {
  box-sizing: border-box;
}

.markdown-body input {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}

.markdown-body a {
  color: #aa9df2;
  text-decoration: none;
}
.markdown-body mjx-container[jax="SVG"] > svg a{fill:#aa9df2;stroke: #aa9df2;}

.markdown-body a:hover {
  text-decoration: underline;
}

.markdown-body strong {
  font-weight: 600;
}

.markdown-body hr:after,
.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body table {
  border-spacing: 0;
  border-collapse: collapse;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body details summary {
  cursor: pointer;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 12px SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  line-height: 12px;
  color: #76dae8;
  vertical-align: middle;
  background-color: #3a2e3f;
  border: 1px solid #504455;
  border-radius: 3px;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body h1 {
  font-size: 32px;
}

.markdown-body h1,
.markdown-body h2 {
  font-weight: 600;
}

.markdown-body h2 {
  font-size: 24px;
}

.markdown-body h3 {
  font-size: 20px;
}

.markdown-body h3,
.markdown-body h4 {
  font-weight: 600;
}

.markdown-body h4 {
  font-size: 16px;
}

.markdown-body h5 {
  font-size: 14px;
}

.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
}

.markdown-body h6 {
  font-size: 12px;
}

.markdown-body p {
  margin-top: 0;
  margin-bottom: 10px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ol ol ol,
.markdown-body ol ul ol,
.markdown-body ul ol ol,
.markdown-body ul ul ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre {
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body input::-webkit-inner-spin-button,
.markdown-body input::-webkit-outer-spin-button {
  margin: 0;
  -webkit-appearance: none;
  appearance: none;
}

.markdown-body:after,
.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  clear: both;
}

.markdown-body>:first-child {
  margin-top: 0!important;
}

.markdown-body>:last-child {
  margin-bottom: 0!important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body blockquote,
.markdown-body details,
.markdown-body dl,
.markdown-body ol,
.markdown-body p,
.markdown-body pre,
.markdown-body table,
.markdown-body ul {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: .25em;
  padding: 0;
  margin: 24px 0;
  background-color: #504455;
  border: 0;
}

.markdown-body blockquote {
  padding: 0 1em;
  color: #c9cdac;
  border-left: .25em solid #f5f5f5;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 24px;
  margin-bottom: 16px;
  font-weight: 600;
  line-height: 1.25;
}

.markdown-body h1 {
  font-size: 2em;
}

.markdown-body h1,
.markdown-body h2 {
  padding-bottom: .3em;
  border-bottom: 1px solid #4a3e4f;
  color: #fed765;
}

.markdown-body h2 {
  font-size: 1.5em;
  color: #fed765;
}

.markdown-body h3 {
  font-size: 1.25em;
  color: #fed765;
}

.markdown-body h4 {
  font-size: 1em;
  color: #fed765;
}

.markdown-body h5 {
  font-size: .875em;
  color: #fed765;
}

.markdown-body h6 {
  font-size: .85em;
  color: #fed765;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 2em;
}

.markdown-body ol ol,
.markdown-body ol ul,
.markdown-body ul ol,
.markdown-body ul ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li {
  word-wrap: break-all;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body li+li {
  margin-top: .25em;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: 600;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
}

.markdown-body table th {
  font-weight: 600;
}

.markdown-body table td,
.markdown-body table th {
  padding: 6px 13px;
  border: 1px solid #786c7d;
}

.markdown-body table tr {
  background-color: #342839;
  border-top: 1px solid #786c7d;
}

.markdown-body table th {
  background-color: #4a3e4f;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #392d3e;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: initial;
}

.markdown-body img[align=right] {
  padding-left: 20px;
}

.markdown-body img[align=left] {
  padding-right: 20px;
}

.markdown-body code {
  padding: .2em .4em;
  margin: 0;
  font-size: 85%;
  background-color: #3a2e3f;
  color: #76dae8;
  border-radius: 3px;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
   font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #3a2e3f;
  border-radius: 3px;
}

.markdown-body pre code {
  display: inline;
  max-width: auto;
  padding: 0;
  margin: 0;
  overflow: visible;
  line-height: inherit;
  word-wrap: normal;
  background-color: initial;
  border: 0;
  color: #f0f0f0;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 .2em .25em -1.6em;
  vertical-align: middle;
}
.markdown-body section.footnotes{
    margin-top:48px;
    border-top:solid 1px #504455;
    padding-top:0px;
}

@media (prefers-color-scheme: dark) {
  .markdown-body mark{color: #111;}
}

/* PrismJS 1.23.0
https://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */


code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background-color: #3a2e3f;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: #48be66;
}

.token.punctuation {
    color: #fdd664;
}

.token.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #9a95e3;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #fdd664;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #$$codeBlockColor$$;
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #ccddf6;
}

.token.function,
.token.class-name {
    color: #f28d55;
}

.token.regex,
.token.important,
.token.variable {
    color: #d38e63;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
  position: relative;
  padding-left: 3.8em;
  counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
  position: relative;
  white-space: inherit;
}

.line-numbers .line-numbers-rows {
  position: absolute;
  pointer-events: none;
  top: 0;
  font-size: 100%;
  left: -3.8em;
  width: 3em; /* works for line-numbers below 1000 lines */
  letter-spacing: -1px;
  border-right: 1px solid #726677;

  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;

}

  .line-numbers-rows > span {
    display: block;
    counter-increment: linenumber;
  }

    .line-numbers-rows > span:before {
      content: counter(linenumber);
      color: #726677;
      display: block;
      padding-right: 0.8em;
      text-align: right;
    }


</style><style>.mweb-charts{background:#fff;}
body{ box-sizing: border-box;
    margin: 0 auto;
    padding: 28px}
@media print{
    pre, code, pre code {
     overflow: visible !important;
     white-space: pre-wrap !important;       /* css-3 */
     white-space: -moz-pre-wrap !important;  /* Mozilla, since 1999 */
     white-space: -pre-wrap !important;      /* Opera 4-6 */
     white-space: -o-pre-wrap !important;    /* Opera 7 */
     word-wrap: break-word !important;       /* Internet Explorer 5.5+ */
    }
    html,body{margin:0;padding:4px;}
}

</style><script>window.MathJax = {     tex: { tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] },     startup: {     pageReady() {       return MathJax.startup.defaultPageReady().then(function () {          window.mweb_mathjax_ready_val = 'yes';          if(window.mweb_mathjax_ready !== undefined){ mweb_mathjax_ready(); }       });     }   }};document.addEventListener('DOMContentLoaded', function(event) {    if (typeof Prism != 'undefined') {         Prism.highlightAll();     }});window.mweb_mathjax_ready_val = '';function theMWebMathJaxRenderIsReady(key){ return window.mweb_mathjax_ready_val; }</script><script>window.MathJax = { tex: { tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] } }; </script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script></head><body><div id='markdown_content' class='markdown-body'><h1><a id="chapter-3-gradient-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Chapter 3 Gradient descent</h1>
<ul>
<li>We assume existance of minimizer \(\mathbf{x}^{\star}\).</li>
<li>The goal is to find approximation \(\mathbf{x}\), s.t. \(f(\mathbf{x})-f\left(\mathbf{x}^{\star}\right)&lt;\varepsilon\).
<ul>
<li>no need for \(\Vert \mathbf{x} - \mathbf{x}^{\star}\Vert\) to be close</li>
</ul>
</li>
</ul>
<h3><a id="notation-on-norm" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notation on Norm</h3>
<ul>
<li>
<p>\(\Vert \cdot\Vert _{a\ast}\) is the dual norm, \(\Vert u\Vert _{a\ast}=\sup _{v \neq 0} \frac{u^{T} v}{\Vert v\Vert _a}=\sup _{\Vert v\Vert _a=1} u^{T} v\)</p>
</li>
<li>
<p>This implies generalized Cauchy-Schwarz inequality \(\lvert u^{T} v\rvert \leq\Vert u\Vert _{a\ast}\Vert v\Vert _a, \forall u,v\).</p>
</li>
<li>
<p>For Eculidiean norm \(\Vert \cdot\Vert =\Vert \cdot\Vert _{\ast}=\Vert \cdot\Vert _{2}\)</p>
</li>
<li>
<p>the parameter \(L\) depends on choice of norm</p>
</li>
<li>
<p>\(\Vert x\Vert _{2} \leq\Vert x\Vert _{1} \leq \sqrt{n}\Vert x\Vert _{2}\), \(\frac{1}{\sqrt{n}}\Vert x\Vert _{2} \leq\Vert x\Vert _{\infty} \leq\Vert x\Vert _{2}\).</p>
</li>
</ul>
<h2><a id="smoothness-and-lipschitz" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothness and Lipschitz</h2>
<h3><a id="gradient-lipschitz" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>(Gradient) Lipschitz</h3>
<ul>
<li><strong>Definition (Lipschitz)</strong> The gradient of \(f\) is <em>Lipschitz continuous</em> with parameter \(L &gt; 0\) w.r.t. norm \(\Vert \cdot\Vert _a\), if \(\Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast} \leq L\Vert x-y\Vert _a\).</li>
<li><strong>Lemma A (generalized Cauchyâ€“Schwarz inequality)</strong> \(\nabla f\) is \(L\)-Lipschitz \(\Rightarrow\) \(\forall \mathbf{x,y}\in\mathbf{dom}(f)\), \((\nabla f(x)-\nabla f(y))^{T}(x-y) \leq L\Vert x-y\Vert ^{2}_{a}\)
<ul>
<li><strong>Proof</strong>
<ul>
<li>\((\nabla f(x)-\nabla f(y))^{T}(x-y) \leq \Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast}\Vert x-y\Vert _{a} \leq L\Vert x-y\Vert _a^2\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="smoothness" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothness</h3>
<ul>
<li>
<p><strong>Definition 3.2 (Smoothness)</strong> Let \(f: \operatorname{dom}(f) \rightarrow \mathbb{R}\) be a differentiable function, \(X \subset \operatorname{dom}(f)\) convex and \(L \in \mathbb{R}_{+}\). Function \(f\) is called <em>smooth with parameter \(L\)</em> over \(X\) (over norm \(\Vert \cdot\Vert _a\)) if \(f(\mathbf{y}) \leq f(\mathbf{x})+\nabla f(\mathbf{x})^{\top}(\mathbf{y}-\mathbf{x})+\frac{L}{2}\Vert \mathbf{x}-\mathbf{y}\Vert _{a}^{2}\)</p>
<ul>
<li>PS: <strong>NO</strong> need to assume \(f\) to be convex</li>
</ul>
</li>
<li>
<p><strong>Lemma 3.3 (equivalance of smoothness in 2-norm)</strong> \(f\) is \(L\)-smooth over 2-norm  \(\Leftrightarrow\) \(g(\mathbf{x})=\frac{L}{2} \mathbf{x}^{\top} \mathbf{x}-f(\mathbf{x})\) is convex over \(\operatorname{dom}(f)\).</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>\(\nabla g(\mathbf{x}) = L\mathbf{x} - \nabla f(\mathbf{x})\)</li>
<li>\(g\) convex \(\Leftrightarrow\) \(\forall \mathbf{x,y}, g(\mathbf{y}) \geq g(\mathbf{x}) + \nabla g(\mathbf{x})^{\top}(\mathbf{y-x})\) \(\Leftrightarrow\) \(\frac{L}{2} \mathbf{y}^{\top} \mathbf{y}-f(\mathbf{y}) \geq \frac{L}{2} \mathbf{x}^{\top} \mathbf{x}-f(\mathbf{x}) + \left[ L\mathbf{x} - \nabla f(\mathbf{x}) \right]^{\top}(\mathbf{y-x})\) \(\Leftrightarrow\) \(f(\mathbf{y}) \leq f(\mathbf{x})+\nabla f(\mathbf{x})^{\top}(\mathbf{y}-\mathbf{x})+\frac{L}{2}\Vert \mathbf{x}-\mathbf{y}\Vert _{2}^{2}\)</li>
</ul>
</li>
<li>If \(f\) twice differentiable, Hessian of \(g\) is \(LI-\nabla^2 f \succeq 0\), this means \(\forall x, \lambda_{\max }\left(\nabla^{2} f(x)\right) \leq L\).</li>
</ul>
</li>
<li>
<p><strong>Lemma B (quadratic upper bound)</strong> If \(\operatorname{dom} f=\mathbf{R}^{n}\) and \(f\) is \(L\)-smooth over norm \(a\), and has minimizer \(\mathbf{x}^{\star}\), then \(\forall z\), \(\frac{1}{2 L}\Vert \nabla f(z)\Vert _{a\ast}^{2} \leq f(z)-f\left(x^{\star}\right) \leq \frac{L}{2}\lVert z-x^{\star}\rVert_{a}^{2}\)</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>Right-hand inequality by smoothness setting \(y=z, x=x^{\star}\), \(f(\mathbf{z}) \leq f(\mathbf{x}^{\star})+ {\underbrace{\nabla f(\mathbf{x}^{\star})}_{=0}}^{\top}(\mathbf{z}-\mathbf{x}^{\star})+\frac{L}{2}\Vert \mathbf{x}^{\star}-\mathbf{z}\Vert _{a}^{2}\)</li>
<li>Left-hand ineuqality, by smoothness \(\inf _{y} f(y) \leq \inf _{y}\left(f(z)+\nabla f(z)^{T}(y-z)+\frac{L}{2}\Vert y-z\Vert ^{2}\right)\)
<ul>
<li>by seperation of direction and magnitude \(\mathsf{LHS }\leq =\inf _{\Vert v\Vert =1} \inf _{t}\left(f(z)+t \nabla f(z)^{T} v+\frac{L t^{2}}{2}\right) = \inf _{\Vert v\Vert =1}\left(f(z)-\frac{1}{2 L}\left(\nabla f(z)^{T} v\right)^{2}\right)\) by maximum of quadratic function.</li>
<li>By the definition of dual norm, we have \(\inf _{y} f(y) = f(z^{\star}) \leq f(z)-\frac{1}{2 L}\Vert \nabla f(z)\Vert _{\ast}^{2}\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Lemma 3.4 (quadratic function)</strong> quadratic form \(f(\mathbf{x})=\mathbf{x}^{\top} Q \mathbf{x}+\mathbf{b}^{\top} \mathbf{x}+c\) is \(2\Vert Q\Vert _{\mathrm{spec}}\)-smooth over 2-norm.</p>
</li>
<li>
<p>Example of Non-Smooth func: \(f(x) = x^4\) since \(\forall L, y^{4} \leq \frac{L}{2} y^{2}\) does not hold for all \(y\) near \(x=0\).</p>
</li>
<li>
<p><strong>Lemma 3.6 (Operation that preserves smoothness)</strong></p>
<ul>
<li>(i) <em>Linear combination</em> \(f:=\sum_{i=1}^{m} \lambda_{i} f_{i}\), where \(\lambda_i &gt; 0\).</li>
<li>(ii) <em>Affine Transform</em> \(f(A \mathbf{x}+\mathbf{b}))\).</li>
<li><strong>Proof</strong> Straightforward</li>
</ul>
</li>
</ul>
<h3><a id="equivalence-of-smoothness-and-gradient-lipschitz-under-convexity" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Equivalence of Smoothness and Gradient Lipschitz under Convexity</h3>
<ul>
<li><strong>Definition (co-coercivity)</strong> The property of <em>co-coercivity</em> of \(\nabla f\) with parameter \(L\) over norm \(a\) is \(\forall x,y\), \((\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{1}{L}\Vert \nabla f(x)-\nabla f(y)\Vert _{\ast}^{2}\)</li>
<li><strong>Lemma 3.5 (Equivalence of Smoothness and Lipschitz under convexity)</strong> If (i) \(\mathbf{dom}(f) = \mathbb{R}^{d}\) (ii) \(f\) convex and differentiable. Then \(f\) is smooth with parameter \(L\) under norm \(a\) \(\Leftrightarrow\) \(\nabla f\) is Lipschitz continuous with parameter \(L\) under norm \(a\).
<ul>
<li><em>Proof Route</em> <strong>Lipschitzness</strong> -(<em>Lemma A</em>)-&gt; <strong>Smoothness</strong> -(<em>Lemma B</em>)-&gt; <strong>Co-coercivity</strong> -&gt; <strong>Lipschitzness</strong></li>
<li><strong>Proof</strong>
<ul>
<li><strong>Lipschitzness</strong> -(<em>Lemma A</em>)-&gt; <strong>Smoothness</strong>
<ul>
<li>Define \(g(t\in[0,1]) = f(x+ t(y-x))\), (well-defined because of convexity of \(\mathbf{dom}(f)\))</li>
<li>\(g^{\prime}(t)-g^{\prime}(0)=(\nabla f(x+t(y-x))-\nabla f(x))^{T}(y-x)\)</li>
<li>Since \(f\) \(L\)-Lipschitz, by Lemma A, we have  \(\forall \mathbf{x,y}\in\mathbf{dom}(f)\), \((\nabla f(x)-\nabla f(y))^{T}(x-y) \leq L\Vert x-y\Vert ^{2}_{a}\)
<ul>
<li>so that \(g^{\prime}(t)-g^{\prime}(0) \leq t L \Vert x-y\Vert _{a}^2\)</li>
</ul>
</li>
<li>\(f(y)=g(1)=g(0)+\int_{0}^{1} g^{\prime}(t) d t \leq g(0)+g^{\prime}(0)+\frac{L}{2}\Vert x-y\Vert ^{2} =f(x)+\nabla f(x)^{T}(y-x)+\frac{L}{2}\Vert x-y\Vert ^{2}\)</li>
<li>We have smoothness</li>
</ul>
</li>
<li><strong>Smoothness</strong> -(<em>Lemma B</em>)-&gt; <strong>Co-coercivity</strong>
<ul>
<li>Define two function \(f_{x}(z)=f(z)-\nabla f(x)^{T} z, f_{y}(z)=f(z)-\nabla f(y)^{T} z\)</li>
<li>Since we only add first-order term, \(f_x{z}, f_y(z)\) are still convex.</li>
<li>Since \(\nabla f_x(z_1) - \nabla f_x(z_2) = \nabla f(z_1) - \nabla f(z_2)\), \(f_x{z}, f_y(z)\) are still \(L\)-smooth.</li>
<li>Also \(\nabla f_x(z=x) = 0\), by convexity, it reaches it minimum, using Smoothness and left-hand inequality of Lemma B, and taking \(f = f_x, z=y\), we get \(f(y)-f(x)-\nabla f(x)^{T}(y-x) \geq \frac{1}{2 L}\Vert \nabla f(y)-\nabla f(x)\Vert _{a\ast}^{2}\)</li>
<li>Similarly \(f(x)-f(y)-\nabla f(y)^{T}(x-y) \geq \frac{1}{2 L}\Vert \nabla f(y)-\nabla f(x)\Vert _{a\ast}^{2}\)</li>
<li>Adding these together and we get \((\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{1}{L}\Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast}^{2}\), co-coercivity</li>
</ul>
</li>
<li><strong>Co-coercivity</strong> -&gt; <strong>Lipschitzness</strong>
<ul>
<li>\(\frac{1}{L}\Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast}^{2} \leq (\nabla f(x)-\nabla f(y))^{T}(x-y) \leq \Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast}\cdot \Vert x-y\Vert _{a}\)</li>
<li>\(\Rightarrow \Vert \nabla f(x)-\nabla f(y)\Vert _{a\ast} \leq L  \Vert x-y\Vert _{a}\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="smoothness-neq-lipschitz-in-f-itself" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothness \(\neq\) Lipschitz in \(f\) itself</h3>
<ul>
<li><strong>Counter Examples</strong>
<ul>
<li><strong>Lipschitz</strong> \(\nRightarrow\) <strong>Smoothness</strong> \(f(x)= \begin{cases}\vert x\vert^{3 / 2}, &amp; \vert x\vert \leq 1 \\ \frac{3}{2}\vert x\vert-\frac{1}{2}, &amp; \vert x\vert \geq 1\end{cases}\), \(f\) is \(3/2\)-Lipschitz but \(\infty\)-smooth.</li>
<li><strong>Smoothness</strong> \(\nRightarrow\) <strong>Lipschitz</strong> \(f(x)=x^{2}, x \in(-\infty, \infty)\), \(f\) is \(2\)-smooth, but \(\infty\)-Lipschitz.</li>
</ul>
</li>
</ul>
<h2><a id="strong-convexity" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Strong convexity</h2>
<ul>
<li>
<p><strong>Definition and Lemma C (Strong Convexity)</strong> A differentiable function \(f\) is strongly convex with parameter \(\mu\) over norm \(\Vert \cdot\Vert _{a}\) if \(\mathbf{dom}(f)\) is convex and (for following are equivalent)</p>
<ul>
<li>(i) \(f(y) \geq f(x)+\nabla f(x)^{T}(y-x)+\frac{\mu}{2}\Vert y-x\Vert _{a}^{2}, \forall x, y\) (This is also used in text book)</li>
<li>(ii) \(f(\theta x+(1-\theta) y) \leq \theta f(x)+(1-\theta) f(y)-\frac{\mu}{2} \theta(1-\theta)\Vert x-y\Vert _{a}^{2}\), \(\theta \in [0,1]\)</li>
<li>(iii) \(\forall x,y\in\mathbf{dom}(f), g(t) :=f(x+t(y-x))-\frac{m}{2} t^{2}\Vert x-y\Vert ^{2}_{a}\) is convex over \(t\)</li>
<li>(iiii) \((\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \mu\Vert x-y\Vert _{a}^{2}\) (also known as <em>strong monotonicity</em> or <em>coercivity</em> of \(\nabla f\))</li>
<li><strong>Proof of Equivalence</strong>
<ul>
<li>(i) -&gt; (ii)
<ul>
<li>set \(y=\theta x+(1-\theta) y, x= x\) and \(y=\theta x+(1-\theta) y, x= y\), we get
<ul>
<li>(a) \(f(\theta x+(1-\theta) y) \geq f(x)+(1-\theta)\nabla f(x)^{T}(y-x)+(1-\theta)^2\frac{\mu}{2}\Vert y-x\Vert _{a}^{2}\)  and</li>
<li>(b) \(f(\theta x+(1-\theta) y) \geq f(y)-\theta\nabla f(y)^{T}(y-x)+\theta^2\frac{\mu}{2}\Vert y-x\Vert _{a}^{2}\)</li>
</ul>
</li>
<li>\(\theta\times\)(a)\(+(1-\theta)\times\)(b) we get (ii)</li>
</ul>
</li>
<li>(ii) -&gt; (iii)
<ul>
<li>Consider function \(h(t) := f(x + t(y-x))\), by (ii) we have \(h(\theta t_1 + (1-\theta) t_2) \leq \theta h(t_1) + (1-\theta)h(t_2) - \frac{\mu}{2} \theta (1-\theta) (t_1-t_2)^2 \Vert y-x\Vert _{a}^{2}\)</li>
<li>Since \(\theta (1-\theta) (t_1-t_2)^2 = \theta t_1 ^2 + (1-\theta) t_2^2 - (\theta t_1 + (1-\theta) t_2)^2\), we have equivalently
<ul>
<li>\(h(\theta t_1 + (1-\theta) t_2  \frac{\mu}{2} \Vert y-x\Vert _{a}^{2}) - (\theta t_1 + (1-\theta) t_2)^2\leq \theta (h(t_1)- t_1 ^2 \frac{\mu}{2} \Vert y-x\Vert _{a}^{2}) + (1-\theta)(h(t_2)-t_2^2\frac{\mu}{2} \Vert y-x\Vert _{a}^{2})\)</li>
</ul>
</li>
<li>or equivalently, define \(g(t):= h(t) - t^2\frac{\mu}{2} \Vert y-x\Vert _{a}^{2}\), \(g\) is a convex function (along this domain of finite line between \(x,y\))
<ul>
<li>if \(f\) differentiable, then \(g\) also</li>
</ul>
</li>
</ul>
</li>
<li>PS: (iii) -&gt; (ii) straightforward by setting \(t=\theta\), and compare with \(t=0,1\).</li>
<li>(iii) -&gt; (i)
<ul>
<li>The first order condition of \(g\) convex gives \(g(1) \geq g(0) + g'(0)\)
<ul>
<li>\(\Rightarrow f(y) - \frac{\mu}{2} \Vert y-x\Vert _{a}^{2} \geq f(x) + \nabla f(x)^{\top}(y-x) - \mu \Vert y-x\Vert _{a}^{2}\), this is (i)</li>
</ul>
</li>
</ul>
</li>
<li>(i) -&gt; (iiii) switch between \(x,y\) and add them together.</li>
<li>(iiii) -&gt; (iii)
<ul>
<li>let \(x=x, y=x+t(y-x)\) in (iiii), and we have \((\nabla f(x + t (y-x) )-\nabla f(x))^{T}(y-x) \geq \mu t \Vert x-y\Vert ^{2}\)</li>
<li>now we compute \(g'(t)\) in (iii), we have \(g'(t) = \nabla f(x + t(y-x))^{\top}(y-x) - \mu t \Vert y-x\Vert _{a}^2\)</li>
<li>assume \(t_1 \geq t_2\), then \(g'(t_1) - g'(t_2) = \nabla (f(x + t_1(y-x)) - f(x + t_2(y-x)))^{\top}(y-x) - \mu (t_1 - t_2) \Vert y-x\Vert _{a}^2\) \(\geq \mu (t1 - t2) \Vert x-y\Vert _{a}^2 - \mu (t_1 - t_2) \Vert y-x\Vert _{a}^2 = 0\)</li>
<li>\(g'(t)\) nondecreasing, convex -&gt; (iii).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Lemma D (boundedness and global minimum)</strong> If \(f\) is differentiable and \(\mu\)-strongly convex, then \(f\) have bounded sublevel.</p>
<ul>
<li><strong>Proof</strong>
<ul>
<li>Since \(f\) \(\mu\)-strongly convex, given any \(x,y\), value along this line will go to \(+\infty\) by quadratic function.</li>
<li>suppose \(f^{\leq \alpha}\) non-empty, since \(f\) convex -&gt; continuous, then \(\exists y\) s.t. \(f(y) = \alpha\).</li>
<li>Then  \(\forall x, f(x) \geq f(y) + \nabla f(y)^{\top} (y-x) + \frac{\mu}{2}\Vert y-x\Vert ^2_{a} \geq f(y) - \Vert \nabla f(y)\Vert _{a\ast}\Vert y-x\Vert  + \frac{\mu}{2}\Vert y-x\Vert ^2_{a}\),</li>
<li>If \(\Vert y-x\Vert _{a} \geq 2 \Vert \nabla f(y)\Vert _{a\ast} / \mu\), then \(f(x) \geq f(y) = \alpha\), then \(f^{\leq \alpha}\)  bounded.</li>
</ul>
</li>
<li>This means \(\inf_{x\in\mathbf{dom}(f)} f &gt; -\infty\), if further one of \(f^{\leq \alpha}\) is closed, then \(f\) have a global minimum.</li>
<li>The global minimum is also unique (trivial).</li>
</ul>
</li>
<li>
<p><strong>Lemma E (quadratic lower bound)</strong> If \(f\) is closed (means has closed sublevel sets), and \(\mathbf{x}^{\star}\) is its unique minimizer, then \(\forall z\in \mathbf{dom}(f), \frac{m}{2}\lVert z-x^{\star}\rVert^{2} \leq f(z)-f\left(x^{\star}\right) \leq \frac{1}{2 m}\Vert \nabla f(z)\Vert _{\ast}^{2}.\)</p>
<ul>
<li><strong>Proof</strong> similar to smoothness case.</li>
</ul>
</li>
<li>
<p><strong>Lemma 3.11 (equivalance of strong convexity in 2-norm)</strong> \(f\) \(\mu\)-strong convex over 2-norm iff \(k(x)=f(x)-\frac{\mu}{2}\Vert x\Vert ^{2}_2\) is convex</p>
<ul>
<li><strong>Proof</strong> similar to smoothness case.</li>
<li>If \(f\) twice differentiable, Hessian of \(h\) is \(\nabla^2 f - \mu I \succeq 0\), this means \(\forall x, \lambda_{\min }\left(\nabla^{2} f(x)\right) \geq \mu\).
<ul>
<li>Strictly Convex.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a id="smooth-and-strong-convexity" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smooth and Strong convexity</h3>
<ul>
<li><strong>Lemma F (Extension of co-coercivity)</strong> Suppose \(f\) is \(\mu\)-strongly convex and \(L\)-smooth for 2-norm, and \(\mathbf{dom}(f) = \mathbb{R}^n\)
<ul>
<li>then function \(h(x)=f(x)-\frac{\mu}{2}\Vert x\Vert _{2}^{2}\) is \(L-\mu\)-smooth</li>
<li>co-coercivity of \(\nabla h\) gives \(\forall x,y, (\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{\mu L}{\mu+L}\Vert x-y\Vert _{2}^{2}+\frac{1}{\mu+L}\Vert \nabla f(x)-\nabla f(y)\Vert _{2}^{2}\)</li>
</ul>
</li>
</ul>
<h2><a id="speed-metric" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Speed metric</h2>
<ul>
<li>\(\lim _{k \rightarrow \infty} \frac{f\left(w^{k+1}\right)-f\left(w^{\ast}\right)}{(f\left(w^{k}\right)-f\left(w^{\ast}\right))^p}=q\)
<ul>
<li>\(p=1, q\in(0,1)\), <em>linear</em> rate. E.g. \(\Delta f_t = O(e^{-\alpha t}), \alpha &gt; 0\).</li>
<li>\(p=1, q=1\), <em>sublinear</em> rate. E.g. \(\Delta f_t = O(t^{-\beta}), \beta &gt; 0\).</li>
<li>\(p=1, q=0\), <em>super linear</em> rate. E.g. \(\Delta f_t = O(e^{-\alpha t^2}), \alpha &gt; 0\).</li>
<li>\(p&gt;1, q&gt;0\), <em>convergence of order \(p\)</em>. E.g. \(\Delta f_t = O(e^{-\alpha p^t}), \alpha &gt; 0\).
<ul>
<li>when \(p=2\), <em>quadratic convergence</em>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a id="vanilla-gd" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vanilla GD</h2>
<ul>
<li>Update: \(\mathbf{x}_{t+1}:=\mathbf{x}_{t}-\gamma \mathbf{g}_{t}\), \(\mathbf{g}_{t}:=\nabla f\left(\mathbf{x}_{t}\right)\)</li>
<li>Consider quantity \(\mathbf{g}_{t}^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right)=\frac{1}{\gamma}\left(\mathbf{x}_{t}-\mathbf{x}_{t+1}\right)^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right) = \frac{1}{2 \gamma}\left(\lVert\mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert^{2}+\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}-\lVert\mathbf{x}_{t+1}-\mathbf{x}^{\star}\rVert^{2}\right)\) \(= \frac{\gamma}{2}\lVert\mathbf{g}_{t}\rVert^{2}+\frac{1}{2 \gamma}\left(\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}-\lVert\mathbf{x}_{t+1}-\mathbf{x}^{\star}\rVert^{2}\right)\)</li>
<li>sum over \(t=0,\ldots,T-1\), we have \(\sum_{t=0}^{T-1} \mathbf{g}_{t}^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right)=\frac{\gamma}{2} \sum_{t=0}^{T-1}\lVert\mathbf{g}_{t}\rVert^{2}+\frac{1}{2 \gamma}\left(\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}-\lVert\mathbf{x}_{T}-\mathbf{x}^{\star}\rVert^{2}\right) \leq =\frac{\gamma}{2} \sum_{t=0}^{T-1}\lVert\mathbf{g}_{t}\rVert^{2}+\frac{1}{2 \gamma}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\)</li>
<li>By convexity \(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right) \leq \mathbf{g}_{t}^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right)\), we have \(\sum_{t=0}^{T-1}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \frac{\gamma}{2} \sum_{t=0}^{T-1}\lVert\mathbf{g}_{t}\rVert^{2}+\frac{1}{2 \gamma}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\)
<ul>
<li>This gives upper bound on average error \(\mathbb{E}_{t} [f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)]\)</li>
<li>note that last iterate is not necessarily the best one. Some algorithms guarantee last iterate convergence.</li>
<li>Bad result since we cannot control \(\Vert \mathbf{g}_t\Vert\)</li>
</ul>
</li>
</ul>
<h2><a id="improvement-case-1-lipschitz-f-mathcal-o-left-1-varepsilon-2-right-steps" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improvement Case 1: Lipschitz \(f\): \(\mathcal{O}\left(1 / \varepsilon^{2}\right)\) Steps</h2>
<ul>
<li>By Theorem 2.9, if we assume convex function \(f\) is \(B\)-Lipschitz, then \(\Vert D f(x)\Vert  \leq B\), the spectual norm in Thm 2.9 is Euclidian norm when \(D f \in \mathbb{R}^{1\times d}\)
<ul>
<li>then \(\Vert\mathbf{g}_t\Vert \leq B\) -&gt; Theorem 3.1</li>
</ul>
</li>
<li><strong>Theorem 3.1</strong> Suppose \(f \in C^1(x)\) convex, with global minimum \(\mathbf{x}^{\star}\). If (1) \(\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert \leq R\), (2) \(\forall \mathbf{x}, \Vert \nabla f(\mathbf{x})\Vert  \leq B\), when with step size \(\gamma:=\frac{R}{B \sqrt{T}}\), we have average error bounded by \(\frac{1}{T} \sum_{t=0}^{T-1}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \frac{R B}{\sqrt{T}}\)
<ul>
<li><strong>Proof</strong> Straighforward from vanilla case, \(\sum_{t=0}^{T-1}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \frac{\gamma}{2} B^{2} T+\frac{1}{2 \gamma} R^{2}\)</li>
<li><strong>Time Complexity</strong> To achieve \(\min _{t=0}^{T-1}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \varepsilon\), we need \(T \geq \frac{R^{2} B^{2}}{\varepsilon^{2}}\) iterations, \(\mathcal{O}\left(1 / \varepsilon^{2}\right)\).</li>
<li>but no dependence on dimension \(d\).</li>
</ul>
</li>
</ul>
<h2><a id="improvement-case-2-smooth-convex-f-mathcal-o-left-1-varepsilon-right-steps" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improvement Case 2: Smooth Convex \(f\): \(\mathcal{O}\left(1 / \varepsilon\right)\) Steps</h2>
<ul>
<li><strong>Lemma 3.7 (Sufficient Descent)</strong> Suppose \(f: \mathbb{R}^{d} \rightarrow \mathbb{R} \in C^{1}\) is \(L\)-smooth. Setting \(\gamma := L^{-1}\), then gradient descent gives \(f\left(\mathbf{x}_{t+1}\right) \leq f\left(\mathbf{x}_{t}\right)-\frac{1}{2 L}\lVert\nabla f\left(\mathbf{x}_{t}\right)\rVert^{2}\). (<strong>Proof</strong> Straightforward)</li>
<li><strong>Lemma 3.8 (Last Iteration Bound for smooth convex funciton)</strong> Suppose \(f: \mathbb{R}^{d} \rightarrow \mathbb{R} \in C^{1}\) is \(L\)-smooth with global minimum \(\mathbf{x}^{\star}\). With step size \(\gamma := L^{-1}\) gradient descent gives \(f\left(\mathbf{x}_{T}\right)-f\left(\mathbf{x}^{\star}\right) \leq \frac{L}{2 T}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\).
<ul>
<li><strong>Proof</strong>
<ul>
<li>Sum <em>sufficient descent</em> over \(t=0:T-1\), we get \(\frac{1}{2 L} \sum_{t=0}^{T-1}\lVert\nabla f\left(\mathbf{x}_{t}\right)\rVert^{2} \leq \sum_{t=0}^{T-1}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}_{t+1}\right)\right)=f\left(\mathbf{x}_{0}\right)-f\left(\mathbf{x}_{T}\right)\)</li>
<li>Take this into vanilla GD bound, we get \(\sum_{t=1}^{T}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \frac{L}{2}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\).</li>
<li>By sufficient descent, monotocity of \(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\) is guaranteed, so \(f\left(\mathbf{x}_{T}\right)-f\left(\mathbf{x}^{\star}\right) \leq \frac{1}{T} \sum_{t=1}^{T}\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) \leq \frac{L}{2 T}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\)</li>
</ul>
</li>
<li><strong>Time Complexity</strong> \(T \geq \frac{R^{2} L}{2 \varepsilon} = \mathcal{O}\left(1 / \varepsilon\right)\)</li>
</ul>
</li>
</ul>
<h2><a id="improvement-case-3-acceleration-for-smooth-f-mathcal-o-1-sqrt-varepsilon-steps" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improvement Case 3: Acceleration for smooth \(f\): \(\mathcal{O}(1 / \sqrt{\varepsilon})\) Steps</h2>
<ul>
<li><strong>Algorithm</strong>
<ul>
<li>\(\mathbf{y}_{t+1}:=\mathbf{x}_{t}-\frac{1}{L} \nabla f\left(\mathbf{x}_{t}\right)\)</li>
<li>\(\mathbf{z}_{t+1}:=\mathbf{z}_{t}-\frac{t+1}{2 L} \nabla f\left(\mathbf{x}_{t}\right)\)</li>
<li>\(\mathbf{x}_{t+1}:=\frac{t+1}{t+3} \mathbf{y}_{t+1}+\frac{2}{t+3} \mathbf{z}_{t+1}\)</li>
</ul>
</li>
<li><strong>Theorem (Nesterov Acceleration)</strong> Suppose \(f\in C^{1}(\mathbb{R}^{d} \rightarrow \mathbb{R})\) is convex  and \(L\)-smooth with global minimum \(\mathbf{x}^{\star}\). Then Nesterov's Accelerated gradient descent algorithm gives \(f\left(\mathbf{y}_{T}\right)-f\left(\mathbf{x}^{\star}\right) \leq \frac{2 L\lVert\mathbf{z}_{0}-\mathbf{x}^{\star}\rVert^{2}_{2}}{T(T+1)}\).
<ul>
<li><strong>Proof</strong> (<em>potential function argument</em>)
<ul>
<li>Define potential function \(\Phi(t):=t(t+1)\left(f\left(\mathbf{y}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right)+2 L\lVert\mathbf{z}_{t}-\mathbf{x}^{\star}\rVert_{2}^{2}\), we aim to show that \(\Phi(T) \leq \Phi(0)\), then we can prove this theorem.</li>
<li>Define \(\Delta:=\frac{\Phi(t+1)-\Phi(t)}{t+1}\), and we need to show that it's less than zero
<ul>
<li>\(\Delta  =  \frac{1}{t+1}\left[(t+2)(t+1)\left(f\left(\mathbf{y}_{t+1}\right)-f\left(\mathbf{x}^{\star}\right)\right) - t(t+1)\left(f\left(\mathbf{y}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right)  + 2 L(\lVert\mathbf{z}_{t+1}-\mathbf{x}^{\star}\rVert^{2}_{2} - \lVert\mathbf{z}_{t}-\mathbf{x}^{\star}\rVert^{2}_{2})\right]\)</li>
<li>\(=t\left(f\left(\mathbf{y}_{t+1}\right)-f\left(\mathbf{y}_{t}\right)\right)+2\left(f\left(\mathbf{y}_{t+1}\right)-f\left(\mathbf{x}^{\star}\right)\right)+\frac{2 L}{t+1}\left(\lVert\mathbf{z}_{t+1}-\mathbf{x}^{\star}\rVert_{2}^{2}-\lVert\mathbf{z}_{t}-\mathbf{x}^{\star}\rVert_{2}^{2}\right)\)</li>
<li>Since \(\mathbf{g}_{t}=\frac{2L}{t+1}(\mathbf{z}_{t} - \mathbf{z}_{t+1})\), we consider \(\mathbf{g}_{t}^{\top}\left(\mathbf{z}_{t}-\mathbf{x}^{\star}\right) = \frac{2L}{t+1}(\mathbf{z}_{t} - \mathbf{z}_{t+1})^{\top}(\mathbf{z}_{t}-\mathbf{x}^{\star})\)</li>
<li>by \(v^{\top}w = \frac{\Vert v\Vert _{2}^2 + \Vert w\Vert _{2}^2 - \Vert v-w\Vert _{2}^2}{2}\), we have \(\mathbf{g}_{t}^{\top}\left(\mathbf{z}_{t}-\mathbf{x}^{\star}\right) = \frac{t+1}{4L} \Vert \mathbf{g}_t\Vert _{2}^2  +    \frac{L}{t+1}\left( \Vert \mathbf{z}_{t}-\mathbf{x}^{\star}\Vert _{2}^2 - \Vert \mathbf{z}_{t+1}-\mathbf{x}^{\star}\Vert _{2}^2 \right)\)</li>
<li>plug this into \(\Delta\) we get \(\Delta = t\left(f\left(\mathbf{y}_{t+1}\right)-f\left(\mathbf{y}_{t}\right)\right)+2\left(f\left(\mathbf{y}_{t+1}\right)-f\left(\mathbf{x}^{\star}\right)\right) - 2\mathbf{g}_{t}^{\top}\left(\mathbf{z}_{t}-\mathbf{x}^{\star}\right)  + \frac{t+1}{2L} \Vert \mathbf{g}_t\Vert ^2_{2}\)</li>
<li>By sufficient descent of \(y_{t+1}\), \(f(y_{t+1})\leq f(x_{t})- \frac{1}{2L}\Vert \mathbf{g}_t\Vert ^2_{2}\), we replace \(f(y_{t+1})\) w.r.t \(f(x_{t})\) and ommit term \(- \frac{1}{2L} \Vert \mathbf{g}_t\Vert ^2_{2}\)
<ul>
<li>\(\Delta \leq t\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{y}_{t}\right)\right)+2\left(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)\right) - 2\mathbf{g}_{t}^{\top}\left(\mathbf{z}_{t}-\mathbf{x}^{\star}\right)\)</li>
<li></li>
</ul>
</li>
<li>By convexity, \(\Delta \leq t\mathbf{g}_{t}^{\top}(\mathbf{x}_{t} - \mathbf{y}_t) + 2\mathbf{g}_{t}^{\top}(\mathbf{x}_{t} - \mathbf{x}^{\star}) - 2\mathbf{g}_{t}^{\top}\left(\mathbf{z}_{t}-\mathbf{x}^{\star}\right) = \mathbf{g}_{t}^{\top}\left[ (t+2)\mathbf{x}_{t} - t \mathbf{y}_t  - 2\mathbf{z}_{t} \right] = 0\) by definition of \(\mathbf{x}_{t}\) .</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a id="improvement-case-4-smooth-and-strongly-convex-f-mathcal-o-log-1-varepsilon-steps-linear-rate" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improvement Case 4: Smooth and Strongly convex \(f\): \(\mathcal{O}(\log (1 / \varepsilon))\) Steps, linear rate</h2>
<ul>
<li><strong>Theorem 3.14 (Linear rate for smooth and strongly convex function)</strong> Suppose \(f\in C^{1}(\mathbb{R}^{d} \rightarrow \mathbb{R})\) is \(L\)-smooth and \(\mu\)-strongly convex with global minimum \(\mathbf{x}^{\star}\). Choosing step size of \(\gamma := L^{-1}\), then gradient descent with arbitrary \(\mathbf{x}_0\),
<ul>
<li>(i) Geometric decrease for \(\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}\), \(\lVert\mathbf{x}_{t+1}-\mathbf{x}^{\star}\rVert^{2} \leq\left(1-\frac{\mu}{L}\right)\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}\)</li>
<li>(ii) Exponential decrease for absoulute error \(f\left(\mathbf{x}_{T}\right)-f\left(\mathbf{x}^{\star}\right) \leq \frac{L}{2}\left(1-\frac{\mu}{L}\right)^{T}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\)</li>
<li><strong>Proof</strong>
<ul>
<li>By storng convexity, \(\mathbf{g}_{t}^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right)=\nabla f\left(\mathbf{x}_{t}\right)^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right) \geq f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)+\frac{\mu}{2}\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}\)
<ul>
<li>again by the fact \(\mathbf{g}_{t}^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right) = \frac{1}{\gamma}(\mathbf{x}_{t} - \mathbf{x}_{t+1})^{\top}\left(\mathbf{x}_{t}-\mathbf{x}^{\star}\right) = \frac{1}{2\gamma}(\gamma^2\Vert \mathbf{g}_{t}\Vert ^2 + \Vert \mathbf{x}_{t}-\mathbf{x}^{\star}\Vert ^2 - \Vert \mathbf{x}_{t+1}-\mathbf{x}^{\star}\Vert ^2)\)</li>
<li>so that \(f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right) \leq \frac{1}{2\gamma}(\gamma^2\Vert \mathbf{g}_{t}\Vert ^2 + \Vert \mathbf{x}_{t}-\mathbf{x}^{\star}\Vert ^2 - \Vert \mathbf{x}_{t+1}-\mathbf{x}^{\star}\Vert ^2) - \frac{\mu}{2}\lVert\mathbf{x}_{t}-\mathbf{x}^{\star}\rVert^{2}\)</li>
<li>Equivalently, \(\Vert \mathbf{x}_{t+1}-\mathbf{x}^{\star}\Vert ^2 \leq (1-\mu\gamma)\Vert \mathbf{x}_{t}-\mathbf{x}^{\star}\Vert ^2 + \gamma^2\Vert \mathbf{g}_{t}\Vert ^2 - 2\gamma (f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right))\)</li>
</ul>
</li>
<li>By sufficient decrease, \(f\left(\mathbf{x}^{\star}\right)-f\left(\mathbf{x}_{t}\right) \leq f\left(\mathbf{x}_{t+1}\right)-f\left(\mathbf{x}_{t}\right) \leq-\frac{1}{2 L}\lVert\nabla f\left(\mathbf{x}_{t}\right)\rVert^{2}\)
<ul>
<li>so that \(\gamma^2\Vert \mathbf{g}_{t}\Vert ^2 - 2\gamma (f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}^{\star}\right)) \leq 0\)</li>
</ul>
</li>
<li>Therefore, \(\Vert \mathbf{x}_{t+1}-\mathbf{x}^{\star}\Vert ^2 \leq (1-\mu\gamma)\Vert \mathbf{x}_{t}-\mathbf{x}^{\star}\Vert ^2 = (1-\frac{\mu}{L})\Vert \mathbf{x}_{t}-\mathbf{x}^{\star}\Vert ^2\)</li>
<li>Iteratively, \(\lVert\mathbf{x}_{T}-\mathbf{x}^{\star}\rVert^{2} \leq\left(1-\frac{\mu}{L}\right)^{T}\lVert\mathbf{x}_{0}-\mathbf{x}^{\star}\rVert^{2}\)</li>
<li>By Strong convexity, \(f\left(\mathbf{x}_{T}\right)-f\left(\mathbf{x}^{\star}\right) \leq \nabla f\left(\mathbf{x}^{\star}\right)^{\top}\left(\mathbf{x}_{T}-\mathbf{x}^{\star}\right)+\frac{L}{2}\lVert\mathbf{x}_{T}-\mathbf{x}^{\star}\rVert^{2}=\frac{L}{2}\lVert\mathbf{x}_{T}-\mathbf{x}^{\star}\rVert^{2}\).</li>
</ul>
</li>
<li><strong>Time Complexiy</strong> \(T \geq \frac{L}{\mu} \ln \left(\frac{R^{2} L}{2 \varepsilon}\right) = \mathcal{O}(\log (1 / \varepsilon))\)</li>
</ul>
</li>
</ul>
</div></body></html>