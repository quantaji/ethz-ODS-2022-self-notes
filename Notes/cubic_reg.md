## Global analysis for strongly-convex smooth objectives

## Overcoming the local nature of Newton method

## Cubic Regularization (Nesterov & Polyak, 2006)
- **motivation** GD can be viewed as iteratively minimizing the quadratic upper bound function $f(\mathbf{y}) \leq f\left(\mathbf{x}\right)+\nabla f\left(\mathbf{x}\right)^{\top}\left(\mathbf{y}-\mathbf{x}\right)+\frac{L_{1}}{2}\lVert \mathbf{y}-\mathbf{x}\rVert ^{2}$.
- **Lemma A** If $f$ has $L_2$-Lipschitz Hessian w.r.t. some norm, then $\vert f(\mathbf{y}) - f\left(\mathbf{x}\right)-\nabla f\left(\mathbf{x}\right)^{\top}\left(\mathbf{y}-\mathbf{x}\right)-\frac{1}{2}\left(\mathbf{y}-\mathbf{x}\right)^{\top} \nabla^{2} f\left(\mathbf{x}\right)\left(\mathbf{y}-\mathbf{x}\right)\vert \leq \frac{L_{2}}{6}\lVert \mathbf{y}-\mathbf{x}\rVert _{a}^{3}$
    - **Proof**
        - $L_2$-Lipschitz Hessian means $\Vert H(\mathbf{y})-H(\mathbf{y})\Vert _{a,a*}\leq\Vert \mathrm{y}-\mathrm{x}\Vert _{a,a*}$, where matrix norm is $\Vert A\Vert _{a,a*}=\sup _{x \neq 0} \frac{\Vert A x\Vert _{a*}}{\Vert x\Vert _{a}}$, and $\Vert \cdot\Vert _{a*}$ is the dual norm.
        - Define $g(t\in[0,1]):=f(\mathbf{x} + t(\mathbf{y-x}))$, and $\mathbf{z}_t:=\mathbf{x} + t(\mathbf{y-x})$, $g'(t)=\nabla f(z)^{\top}(\mathbf{y-x})$, $g''(t):= (\mathbf{y-x})^{\top}H(\mathbf{z}_{t})(\mathbf{y-x})$
        - Then $\vert g''(t) - g''(0)\vert = \vert(\mathbf{y-x})^{\top}(H(\mathbf{z}_{t}) - H(\mathbf{x}))(\mathbf{y-x}) \vert \leq \Vert (H(\mathbf{z}_{t}) - H(\mathbf{x}))(\mathbf{y-x})\Vert _{a*}\Vert \mathbf{y-x}\Vert _{a}$
        - $\mathsf{RHS}\leq \Vert H(\mathbf{z}_{t}) - H(\mathbf{x})\Vert _{a, a*}\Vert \mathbf{y-x}\Vert _{a}^2$, by Lipschitz $\leq  L_2 t \Vert \mathbf{y-x}\Vert _{a}^3$.
        - Then $g'(t) - g'(0) = \int_{0}^{t} g''(t) d\tau \leq \int_{0}^{t} g''(0) +  \tau \Vert \mathbf{y-x}\Vert _{a}^3 d\tau =g''(0) t + \frac{L_2}{2} \Vert \mathbf{y-x}\Vert _{a}^3t^2$,
            - also $g'(t) - g'(0) \geq \int_{0}^{t} g''(0) -  \tau \Vert \mathbf{y-x}\Vert _{a}^3 d\tau =g''(0) t - \frac{L_2}{2} \Vert \mathbf{y-x}\Vert _{a}^3t^2$
        - Then $g(t)-g(0) = \int_{0}^{t} g'(\tau) d\tau \leq  \int_{0}^{t} g'(0) + g''(0) \tau + \frac{L_2}{2} \Vert \mathbf{y-x}\Vert _{a}^3 \tau^2 d\tau = g'(0) t + \frac{1}{2}g''(0)t^2 + \frac{L_2}{6} \Vert \mathbf{y-x}\Vert _{a}^3 t^3$
            - $g(t)-g(0) \geq  \int_{0}^{t} g'(0) + g''(0) \tau - \frac{L_2}{2} \Vert \mathbf{y-x}\Vert _{a}^3 \tau^2 d\tau = g'(0) t + \frac{1}{2}g''(0)t^2 -\frac{L_2}{6} \Vert \mathbf{y-x}\Vert _{a}^3 t^3$
        - Setting $t=1$, and we get $\vert f(\mathbf{x}) - f\left(\mathbf{x}_{t}\right)-\nabla f\left(\mathbf{x}_{t}\right)^{\top}\left(\mathbf{x}-\mathbf{x}_{t}\right)-\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{t}\right)^{\top} \nabla^{2} f\left(\mathbf{x}_{t}\right)\left(\mathbf{x}-\mathbf{x}_{t}\right)\vert \leq \frac{L_{2}}{6}\lVert \mathbf{x}-\mathbf{x}_{t}\rVert _{a}^{3}$
- **Lemma B** If $f$ has $L_2$-Lipschitz Hessian w.r.t. some norm, then $\Vert \nabla f(\mathbf{y}) - \nabla f\left(\mathbf{x}\right)- \nabla^{2} f\left(\mathbf{x}\right)\left(\mathbf{y}-\mathbf{x}\right)\Vert _{a*} \leq \frac{L_{2}}{2}\lVert \mathbf{y}-\mathbf{x}\rVert _{a}^{2}$
    - **Proof**
        - Similar to Lemma A, but define $\mathbf{k}(t):= \nabla f(\mathbf{x}+t(\mathbf{y-x}))$, then $\partial_t \mathbf{k}=\nabla^{2} f_t \left(\mathbf{y}-\mathbf{x}\right)$,
        - So we have $\Vert \partial_t\mathbf{k}(t) - \partial_t\mathbf{k}(0)\Vert _{a*} \leq L_2 t\Vert \mathbf{y}-\mathbf{x}\Vert _{a}^2$.
        - By triangular ineq, $\Vert \int \Vert _{a*}\leq \int \Vert \Vert _{a*}$, so we get similar result.
- **ALgorithm** (*Subproblem*) $\mathbf{x}_{t+1} \in \underset{\mathbf{x}}{\operatorname{argmin}} \hat{f}\left(\mathbf{x}, \mathbf{x}_{t}\right)$, where $\hat{f}\left(\mathbf{x}, \mathbf{x}_{t}\right):=f\left(\mathbf{x}_{t}\right)+\nabla f\left(\mathbf{x}_{t}\right)^{\top}\left(\mathbf{x}-\mathbf{x}_{t}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{t}\right)^{\top} \nabla^{2} f\left(\mathbf{x}_{t}\right)\left(\mathbf{x}-\mathbf{x}_{t}\right)+\frac{M}{6}\lVert \mathbf{x}-\mathbf{x}_{t}\rVert _{2}^{3}$
- Subproblem can be reduced to a convex problem, and can also be solved directly by GD to global optimal.
- No invertion of Hessian is needed.
- **Lemma C (Graded assignment 4.2)** Given two problem $u(\mathbf{h})=\mathbf{g}^{\top} \mathbf{h}+\frac{1}{2} \mathbf{h}^{\top} H \mathbf{h}+\frac{\mathrm{M}}{6}\Vert \mathbf{h}\Vert ^{3}$ and $v({r})=-\frac{1}{2} \mathbf{g}^{\top}\left(H+\frac{Mr}{2} I_{d}\right)^{-1} \mathrm{~g}-\frac{M}{12} r^{3}$ where $\mathcal{D}=\left\{r \geqslant 0 \mid H+\frac{Mr}{2} I_d \succ 0\right\}$, then $\inf _{\mathbf{h} \in \mathbb{R}^{d}} u(\mathbf{h})=\sup _{r \in \mathcal{D}} v(r)$
    - **Proof**
        - First we prove $\inf_{\mathbf{h}} u(\mathbf{h}) \geqslant \sup_{r} v(r)$
            - First by adding and subtracting the same term $\frac{1}{2}  \mathbf{h}^{\top} (\frac{1}{2} M r I_d) \mathbf{h}$, we have
                - $u(\mathbf{h}) =\mathbf{g}^{\top} \mathbf{h}+\frac{1}{2} \mathbf{h}^{\top} H \mathbf{h} + \frac{1}{2}  \mathbf{h}^{\top} (\frac{1}{2} M r I_d) \mathbf{h} - \frac{1}{2}  \mathbf{h}^{\top} (\frac{1}{2} M r I_d) \mathbf{h} +\frac{\mathrm{M}}{6}\Vert \mathbf{h}\Vert ^{3}$ $= \underline{\mathbf{g}^{\top} \mathbf{h}+\frac{1}{2} \mathbf{h}^{\top} \left( H + \frac{1}{2} M r I_d \right) \mathbf{h}} + \frac{\mathrm{M}}{6}\Vert \mathbf{h}\Vert ^{3} - \frac{M r}{4} \Vert \mathbf{h} \Vert ^2$
            - Now we write the underlined quadratic form into canonical form, $\mathbf{g}^{\top} \mathbf{h}+\frac{1}{2} \mathbf{h}^{\top} \left( H + \frac{1}{2} M r I_d \right) \mathbf{h} =  \frac{1}{2} (\mathbf{h} - h(r))^{\top} \left( H + \frac{1}{2} M r I_d \right)  (\mathbf{h} - h(r)) -\frac{1}{2} \mathbf{g}^{\top}\left(H + \frac{M r}{2}I_d\right)^{-1} \mathbf{g}$, where $h(r)=-\left(H+\frac{M r}{2} I_{d}\right)^{-1} g$
            - Plug this in and we get $u(\mathbf{h}) =  \frac{1}{2} (\mathbf{h} - h(r))^{\top} \left( H + \frac{1}{2} M r I_d \right)  (\mathbf{h} - h(r))  + \underbrace{ \left[-\frac{1}{2} \mathbf{g}^{\top}\left(H + \frac{M r}{2}I_d\right)^{-1} \mathbf{g} - \frac{M}{12} r^{3} \right] }_{v(r)} + \frac{\mathrm{M}}{6}\Vert \mathbf{h}\Vert ^{3} - \frac{M r}{4} \Vert \mathbf{h} \Vert ^2 + \frac{M}{12} r^{3}$ $=v(r) +  \frac{1}{2} (\mathbf{h} - h(r))^{\top} \left( H + \frac{1}{2} M r I_d \right)  (\mathbf{h} - h(r)) + \frac{M}{12} (\Vert \mathbf{h} \Vert  - r)^2(r + 2 \Vert \mathbf{h} \Vert  )$
            - By definition, the last two term are both non-negative, so we prove $\inf_{\mathbf{h}} u(\mathbf{h}) \geqslant \sup_{r} v(r)$.
        - Then we prove when $\mathbf{h}=h(r)$, equality holds. 
            - In this circumstance, the second term is zero, so $u(h(r)) - v(r) = \frac{M}{12} (\Vert  h(r) \Vert  - r)^2(r + 2 \Vert  h(r) \Vert  )$
            - Then we consider the analytical expression of $v(r)$, we expend w.r.t eigen values of $H$,  $H = Q\mathrm{diag}\{\lambda_i\}_{i=1}^{d}Q^{\top} $, where $Q = [\mathbf{q}_{1}, \ldots, \mathbf{q}_{d}]$ is an orthonormal matrix. Then $\frac{1}{2} \mathbf{g}^{\top}\left(\mathrm{H}+\frac{Mr }{2} I_d \right)^{-1} \mathbf{g} =  \frac{1}{2}  \mathbf{g}^{\top}Q \mathrm{diag} \left\{ \frac{1}{Mr/2 + \lambda_i} \right\}_{i=1}^{d} Q^{\top}\mathbf{g} = \frac{1}{2}\sum_{i=1}^{d} \frac{(\mathbf{q}_i^{\top} \mathbf{g})^2}{Mr/2 + \lambda_i}.$
            - Then we have the derivatives of $v(r)$, 
                - (i) $v(r) = -  \frac{1}{2}\sum_{i=1}^{d} \frac{(\mathbf{q}_i^{\top} \mathbf{g})^2}{M r/2 + \lambda_i} - \frac{M}{12} r^3$, 
                - (ii) $v'(r) = \frac{M}{2} \frac{1}{2}\sum_{i=1}^{d} \frac{(\mathbf{q}_i^{\top} \mathbf{g})^2}{(M r/2 + \lambda_i)^2} - \frac{M}{4} r^2 =  \frac{M}{2} \frac{1}{2} \mathbf{g}^{\top} \left( H + \frac{Mr}{2} I_d\right)^{-2} \mathbf{g} - \frac{M}{4} r^2= \frac{M}{4} \left[ \Vert  \left( H + \frac{Mr}{2} I_d\right)^{-1} \mathbf{g}\Vert ^2  - r^2 \right]$ $= \frac{M}{4} (\Vert h(r)\Vert ^2 - r^2)$
                - (iii) $v''(r) = - \frac{M^2}{8} \sum_{i=1}^{d} \frac{(\mathbf{q}_i^{\top} \mathbf{g})^2}{(M r/2 + \lambda_i)^3} - \frac{M}{2} r $
            - By definition $H+\frac{Mr}{2} I_d \succ 0$, so that $M r / 2+\lambda_{i}>0$, and since $\Vert \mathbf{g}\Vert \neq 0$ (otherwise trivial), this means $v''(r)< 0$, strictly concave, local maximual is global.
            - The optimial condition holds when $\lVert h\left(r^{*}\right)\rVert =r^{*}$, when this happens $u(h(r^*)) - v(r^*)  =  u(h(r^*)) - \sup_{r} v(r) = \frac{M}{12} (\Vert  h(r^*) \Vert  - r^*)^2(r^*+ 2 \Vert  h(r^*) \Vert  ) = 0$. QED
    - **Remark** $v(r)$ is a convex program (while $u(\mathbf{h})$ is not), and can be solved by GD.
- **Key Facts 1** Second order $\nabla^{2} f\left(\mathbf{x}_{t}\right)+\frac{M}{2}\lVert \mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert  \cdot I \succeq 0$.
    - **Proof**
        - $\mathbf{x}_{t+1} - \mathbf{x}_{t} = \mathbf{h} = h(r^{\star})$ and $\Vert \mathbf{x}_{t+1} - \mathbf{x}_{t}\Vert _{2} = \Vert h(r^{\star})\Vert _2 = r^{\star}$, so $\nabla^{2} f\left(\mathbf{x}_{t}\right)+\frac{M}{2}\lVert \mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert  = H + \frac{Mr^{\star}}{2}I \succeq 0$ by definition of domain $\mathcal{D}$.

- **Key Facts 2** First order $\lVert \nabla f\left(\mathbf{x}_{t+1}\right)\rVert  \leq \frac{L_{2}+M}{2}\lVert \mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert ^{2}$.
    - **Proof 2**
        - By the first order optimality condition we have $\nabla f(\mathbf{x}_{t}) + \nabla ^2 f(\mathbf{x}_{t})(\mathbf{x}_{t+1} - \mathbf{x}_{t}) + \frac{M}{2}\Vert \mathbf{x}_{t+1} - \mathbf{x}_{t}\Vert _2 \cdot(\mathbf{x}_{t+1} - \mathbf{x}_{t}) = 0$
        - By Lemma B, $\Vert \nabla f(\mathbf{x}_{t+1}) - (f(\mathbf{x}_{t}) + \nabla ^2 f(\mathbf{x}_{t})(\mathbf{x}_{t+1} - \mathbf{x}_{t}))\Vert _{2} \leq \frac{L_2}{2}\Vert \mathbf{x}_{t+1} - \mathbf{x}_{t}\Vert _2^2$,
        - By triangle inequality, we prove the result, $\Vert \nabla f(\mathbf{x}_{t+1})\Vert _{2} \leq \Vert \nabla f(\mathbf{x}_{t+1}) - (f(\mathbf{x}_{t}) + \nabla ^2 f(\mathbf{x}_{t})(\mathbf{x}_{t+1} - \mathbf{x}_{t}))\Vert _{2} + \Vert f(\mathbf{x}_{t}) + \nabla ^2 f(\mathbf{x}_{t})(\mathbf{x}_{t+1} - \mathbf{x}_{t})\Vert _{2}$.
- **Key Facts 3** Zero order $f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}_{t+1}\right) \geq \frac{M}{12}\lVert \mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert ^{3}$ if $M\geq L_2$
    - **Proof**
        - By $L_2$-Lipschitz, $f(x) - (f\left(\mathbf{x}_{t}\right)+\nabla f\left(\mathbf{x}_{t}\right)^{\top}\left(\mathbf{x}-\mathbf{x}_{t}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{t}\right)^{\top} \nabla^{2} f\left(\mathbf{x}_{t}\right)\left(\mathbf{x}-\mathbf{x}_{t}\right)) \leq  \frac{L_2}{6}\lVert \mathbf{x}-\mathbf{x}_{t}\rVert _{2}^{3}$
        - so taking $\mathbf{x=x}_{t+1}$, we have 
            - $f(\mathbf{x}_{t}) - f(\mathbf{x}_{t+1}) \geq -\frac{L_2}{6} r^3 - \mathbf{g}^{\top} \mathbf{h} - \frac{1}{2} \mathbf{h}^{\top} H \mathbf{h} = \frac{M - L_2}{6} r^3 - u(\mathbf{h}) = \frac{M - L_2}{6} r^3 - v(r)$
        - Since $M > L_2$, we have $f(\mathbf{x}_{t}) - f(\mathbf{x}_{t+1}) \geq - v(r) = \frac{1}{2} \mathbf{g}^{\top}\left(H+\frac{Mr}{2} I_{d}\right)^{-1} \mathrm{~g} + \frac{M}{12} r^{3} \geq \frac{M}{12} r^{3}$
- **Implication 1** If $\mathbf{x}^{\star}$ is limiting point, then $\nabla f\left(\mathbf{x}^{*}\right)=0, \nabla^{2} f\left(\mathbf{x}^{*}\right) \succeq 0$, since $r=0$, by Fact 1, $\nabla^{2} f\left(\mathbf{x}^{\star}\right) \succeq 0$, by Fact 2, $\lVert \nabla f\left(\mathbf{x}^{\star}\right)\rVert  =0$.
- **Implication 2** Convergence rate of $\min _{1 \leq i \leq t}\lVert \nabla f\left(\mathbf{x}_{i}\right)\rVert =O\left(\frac{1}{t^{2 / 3}}\right)$, since $f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}_{t+1}\right) \geq \frac{M}{12}\lVert \mathbf{x}_{t}-\mathbf{x}_{t+1}\rVert ^{3} \geq C \cdot \Vert \nabla f(\mathbf{x}_{t+1})\Vert _2^{3/2}$
    - then do sumation, $f\left(\mathbf{x}_{1}\right)-f\left(\mathbf{x}^{\star}\right) \geq C\sum_{i=1}^{t} \Vert \nabla f(\mathbf{x}_{i})\Vert _2^{3/2} \geq C\cdot t \left[ \min_i \Vert \nabla f(\mathbf{x}_{i})\Vert _2 \right]^{3/2}$
- **Implication 3** If $f$ convex, $f\left(\mathbf{x}_{t}\right)-f^{*}=O\left(\frac{1}{t^{2}}\right)$.
    - $f(\mathbf{x}_{t}) - f^{\star} \leq \Vert \nabla f(\mathbf{x}_{t})\Vert \cdot \Vert \mathbf{x}_{t} - \mathbf{x}^{\star}\Vert  \leq \Vert \nabla f(\mathbf{x}_{t})\Vert \cdot \Vert \mathbf{x}_{0} - \mathbf{x}^{\star}\Vert$ (*unproved)*, then $f\left(\mathbf{x}_{t}\right)-f\left(\mathbf{x}_{t+1}\right) \geq C \cdot \Vert \nabla f(\mathbf{x}_{t+1})\Vert _2^{3/2} \geq C' (\Delta f_{t+1})^{3/2}$. This gives $\Delta_{t} -\Delta_{t+1} \geq C \Delta_{t+1}^{3/2}$.  
    - This means$\frac{1}{\sqrt{\Delta_{t+1}}} - \frac{1}{\sqrt{\Delta_{t}}} \geq C \frac{\Delta_{t}}{\sqrt{\Delta_{t+1}}(\sqrt{\Delta_t} + \sqrt{\Delta_{t+1}})} = \frac{C}{\sqrt{\frac{\Delta_{t+1}}{\Delta_{t}}}(1+\sqrt{\frac{\Delta_{t+1}}{\Delta_{t}}})}$
    - Since $0\leq \sqrt{\frac{\Delta_{t+1}}{\Delta_{t}}} \leq 1$, $\sqrt{\frac{\Delta_{t+1}}{\Delta_{t}}}(1+\sqrt{\frac{\Delta_{t+1}}{\Delta_{t}}}) \leq 2$, so $\frac{1}{\sqrt{\Delta_{t+1}}} - \frac{1}{\sqrt{\Delta_{t}}} \geq C/2$ and $\frac{1}{\sqrt{\Delta_{t}}} - \frac{1}{\sqrt{\Delta_{0}}} \geq C t/2$
    - $\Delta_{t} = O(1/t^2)$
